{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Sentiment Classfication.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sinethemba-nongqoto/Research-project/blob/master/Copy_of_Sentiment_Classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00tGKRX_Typw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "42f9efff-8445-4557-f5ba-4736cef36bd0"
      },
      "source": [
        "#connecting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzCJKcoWT1aa"
      },
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJWjUgzb6VXb"
      },
      "source": [
        "# reading data from google drive\n",
        "features_sentiment = pd.read_csv('./drive/My Drive/Sentiment_analysis/Data/Copy of bert_features_sentiment.csv', sep=',', header = None)\n",
        "features_stance = pd.read_csv('./drive/My Drive/Sentiment_analysis/Data/Copy of bert_features_stance.csv', sep=',', header = None)\n",
        "twitter_stance = pd.read_csv('./drive/My Drive/Sentiment_analysis/Data/Copy of twitter_stance.csv', sep=',', header = None)\n",
        "data_sentiment = pd.read_csv('./drive/My Drive/Sentiment_analysis/Data/Copy of data_sentiment.csv', sep=',', header = None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBukBnd5-Kmq"
      },
      "source": [
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKMeoI3x7ic_"
      },
      "source": [
        "sentiment = data[3]\n",
        "sentiment  = np.array(sentiment)\n",
        "#load the vectors embedded by BERT\n",
        "vector_bert = pd.read_csv('./drive/My Drive/Colab Notebooks/bert_features_sentiment.csv', sep=',', header = None)\n",
        "vector_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdlQme113c7n"
      },
      "source": [
        "\n",
        "label_new = [0]*len(sentiment)\n",
        "for i in range(len(sentiment)):\n",
        "  if sentiment[i] == 1:\n",
        "    label_new[i]=0\n",
        "  elif sentiment[i] == 2:\n",
        "    label_new[i]=1\n",
        "  elif sentiment[i] == 3:\n",
        "    label_new[i]=2\n",
        "  elif sentiment[i] == 4:\n",
        "    label_new[i]=3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDyJDT34UGnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46ae2a7-9471-4e42-f8c4-97fd09d1dfdd"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from math import floor\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, LSTM, RepeatVector, Layer\n",
        "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras import objectives\n",
        "\n",
        "class VAE:\n",
        "\tdef __init__(self, input_dim, latent_dim, hidden_dims, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.hidden_dims = hidden_dims\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.optimizer = optimizer\n",
        "\t\tself.epsilon_std = epsilon_std\n",
        "\t\tself.build_model()\n",
        "\n",
        "\tdef build_model(self):\n",
        "\t\tinput_layer = Input(batch_shape=(self.batch_size, self.input_dim))\n",
        "\t\tself.build_encoder(input_layer)\n",
        "\t\tself.build_decoder()\n",
        "\t\tself.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
        "\t\tvae_loss = self._get_vae_loss()\n",
        "\t\tself.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
        "\n",
        "\tdef build_encoder(self, input_layer):\n",
        "\t\tprev_layer = input_layer\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = Dense(q, activation='relu')(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tself._build_z_layers(hidden)\n",
        "\t\tself.encoder = Model(input_layer, self.z_mean)\n",
        "\n",
        "\tdef _build_z_layers(self, hidden_layer):\n",
        "\t\tself.z_mean = Dense(self.latent_dim)(hidden_layer)\n",
        "\t\tself.z_log_sigma = Dense(self.latent_dim)(hidden_layer)\n",
        "\n",
        "\tdef build_decoder(self):\n",
        "\t\tz = self._get_sampling_layer()\n",
        "\t\tprev_layer = z\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = Dense(q, activation='relu')(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tself.x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
        "\n",
        "\t\t# Build the stand-alone generator\n",
        "\t\tgenerator_input = Input((self.latent_dim,))\n",
        "\t\tprev_layer = generator_input\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = Dense(q, activation='relu')(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tgen_x_decoded_mean = Dense(self.input_dim, activation='sigmoid')(prev_layer)\n",
        "\t\tself.generator = Model(generator_input, gen_x_decoded_mean)\n",
        "\n",
        "\tdef _get_sampling_layer(self):\n",
        "\t\tdef sampling(args):\n",
        "\t\t\tz_mean, z_log_sigma = args\n",
        "\t\t\tepsilon = K.random_normal(shape=(self.batch_size, self.latent_dim),\n",
        "\t\t\t\t\t\t\t\t\t  mean=0., stddev=self.epsilon_std)\n",
        "\t\t\treturn z_mean + z_log_sigma * epsilon\n",
        "\t\treturn Lambda(sampling, output_shape=(self.latent_dim,))([self.z_mean, self.z_log_sigma])\n",
        "\n",
        "\tdef _get_vae_loss(self):\n",
        "\t\tz_log_sigma = self.z_log_sigma\n",
        "\t\tz_mean = self.z_mean\n",
        "\t\tdef vae_loss(x, x_decoded_mean):\n",
        "\t\t\treconstruction_loss = objectives.mse(x, x_decoded_mean)\n",
        "\t\t\tkl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
        "\t\t\treturn reconstruction_loss + kl_loss\n",
        "\n",
        "\t\treturn vae_loss\n",
        "\n",
        "\n",
        "class VAE_LSTM(VAE):\n",
        "\tdef __init__(self, input_dim, latent_dim, hidden_dims, timesteps, batch_size, optimizer='rmsprop', epsilon_std = .01):\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.hidden_dims = hidden_dims\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.timesteps = timesteps\n",
        "\t\tself.optimizer = optimizer\n",
        "\t\tself.epsilon_std = epsilon_std\n",
        "\t\tself.build_model()\n",
        "\n",
        "\tdef build_model(self):\n",
        "\t\tinput_layer = Input(shape=(self.timesteps, self.input_dim,))\n",
        "\t\tself.build_encoder(input_layer)\n",
        "\t\tself.build_decoder()\n",
        "\t\tself.autoencoder = Model(input_layer, self.x_decoded_mean)\n",
        "\t\tvae_loss = self._get_vae_loss()\n",
        "\t\tself.autoencoder.compile(optimizer=self.optimizer, loss=vae_loss)\n",
        "\n",
        "\tdef build_encoder(self, input_layer):\n",
        "\t\tprev_layer = input_layer\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = LSTM(q)(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tself._build_z_layers(hidden)\n",
        "\t\tself.encoder = Model(input_layer, self.z_mean)\n",
        "\n",
        "\tdef build_decoder(self):\n",
        "\t\tz = self._get_sampling_layer()\n",
        "\t\tprev_layer = RepeatVector(self.timesteps)(z)\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = LSTM(q, return_sequences=True)(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tself.x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
        "\n",
        "\t\t# Build the stand-alone generator\n",
        "\t\tgenerator_input = Input((self.latent_dim,))\n",
        "\t\tprev_layer = RepeatVector(self.timesteps)(generator_input)\n",
        "\t\tfor q in self.hidden_dims:\n",
        "\t\t\thidden = LSTM(q, return_sequences=True)(prev_layer)\n",
        "\t\t\tprev_layer = hidden\n",
        "\t\tgen_x_decoded_mean = LSTM(self.input_dim, return_sequences=True)(prev_layer)\n",
        "\t\tself.generator = Model(generator_input, gen_x_decoded_mean)\n",
        "  \n",
        "\n",
        "N = len(vector_bert)\n",
        "train = np.array(vector_bert)\n",
        "train = train.reshape([N,1,1024])\n",
        "\n",
        "batch_size = 50\n",
        "epochs = 300\n",
        "input_dim = train.shape[-1]\n",
        "timesteps = train.shape[1]\n",
        "\n",
        "model = VAE_LSTM(input_dim=input_dim, latent_dim=100, hidden_dims=[32], timesteps=timesteps, batch_size=batch_size)\n",
        "vae, encoder, generator = model.autoencoder, model.encoder, model.generator\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "vae.fit(train[:floor(N/batch_size)*batch_size],train[:floor(N/batch_size)*batch_size], shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(train[N-1-batch_size:N-1],train[N-1-batch_size:N-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Train on 6100 samples, validate on 50 samples\n",
            "Epoch 1/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3489 - val_loss: 0.3816\n",
            "Epoch 2/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3234 - val_loss: 0.3734\n",
            "Epoch 3/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3192 - val_loss: 0.3712\n",
            "Epoch 4/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3178 - val_loss: 0.3700\n",
            "Epoch 5/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3171 - val_loss: 0.3699\n",
            "Epoch 6/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3167 - val_loss: 0.3697\n",
            "Epoch 7/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3164 - val_loss: 0.3696\n",
            "Epoch 8/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3163 - val_loss: 0.3695\n",
            "Epoch 9/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3162 - val_loss: 0.3695\n",
            "Epoch 10/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3158 - val_loss: 0.3693\n",
            "Epoch 11/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3142 - val_loss: 0.3684\n",
            "Epoch 12/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3136 - val_loss: 0.3685\n",
            "Epoch 13/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3134 - val_loss: 0.3684\n",
            "Epoch 14/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3133 - val_loss: 0.3682\n",
            "Epoch 15/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3132 - val_loss: 0.3683\n",
            "Epoch 16/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3131 - val_loss: 0.3683\n",
            "Epoch 17/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3129 - val_loss: 0.3681\n",
            "Epoch 18/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3126 - val_loss: 0.3671\n",
            "Epoch 19/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3122 - val_loss: 0.3664\n",
            "Epoch 20/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3120 - val_loss: 0.3660\n",
            "Epoch 21/300\n",
            "6100/6100 [==============================] - 16s 3ms/step - loss: 0.3119 - val_loss: 0.3658\n",
            "Epoch 22/300\n",
            "6100/6100 [==============================] - 14s 2ms/step - loss: 0.3117 - val_loss: 0.3656\n",
            "Epoch 23/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3116 - val_loss: 0.3653\n",
            "Epoch 24/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3113 - val_loss: 0.3651\n",
            "Epoch 25/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3110 - val_loss: 0.3650\n",
            "Epoch 26/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3107 - val_loss: 0.3647\n",
            "Epoch 27/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3105 - val_loss: 0.3644\n",
            "Epoch 28/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3103 - val_loss: 0.3640\n",
            "Epoch 29/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3100 - val_loss: 0.3637\n",
            "Epoch 30/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3097 - val_loss: 0.3632\n",
            "Epoch 31/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3093 - val_loss: 0.3629\n",
            "Epoch 32/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3090 - val_loss: 0.3625\n",
            "Epoch 33/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3088 - val_loss: 0.3623\n",
            "Epoch 34/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3087 - val_loss: 0.3623\n",
            "Epoch 35/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3086 - val_loss: 0.3620\n",
            "Epoch 36/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3085 - val_loss: 0.3619\n",
            "Epoch 37/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3084 - val_loss: 0.3618\n",
            "Epoch 38/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3083 - val_loss: 0.3616\n",
            "Epoch 39/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3081 - val_loss: 0.3613\n",
            "Epoch 40/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3079 - val_loss: 0.3610\n",
            "Epoch 41/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3076 - val_loss: 0.3610\n",
            "Epoch 42/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3073 - val_loss: 0.3606\n",
            "Epoch 43/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3071 - val_loss: 0.3605\n",
            "Epoch 44/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3069 - val_loss: 0.3603\n",
            "Epoch 45/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3067 - val_loss: 0.3601\n",
            "Epoch 46/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3066 - val_loss: 0.3599\n",
            "Epoch 47/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3064 - val_loss: 0.3597\n",
            "Epoch 48/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3063 - val_loss: 0.3596\n",
            "Epoch 49/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3062 - val_loss: 0.3596\n",
            "Epoch 50/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3060 - val_loss: 0.3592\n",
            "Epoch 51/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3059 - val_loss: 0.3590\n",
            "Epoch 52/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3057 - val_loss: 0.3588\n",
            "Epoch 53/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3055 - val_loss: 0.3585\n",
            "Epoch 54/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3054 - val_loss: 0.3582\n",
            "Epoch 55/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3052 - val_loss: 0.3580\n",
            "Epoch 56/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3051 - val_loss: 0.3579\n",
            "Epoch 57/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3049 - val_loss: 0.3578\n",
            "Epoch 58/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3048 - val_loss: 0.3576\n",
            "Epoch 59/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3047 - val_loss: 0.3575\n",
            "Epoch 60/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3045 - val_loss: 0.3573\n",
            "Epoch 61/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3044 - val_loss: 0.3571\n",
            "Epoch 62/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3042 - val_loss: 0.3571\n",
            "Epoch 63/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3041 - val_loss: 0.3568\n",
            "Epoch 64/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3040 - val_loss: 0.3566\n",
            "Epoch 65/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3039 - val_loss: 0.3565\n",
            "Epoch 66/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3038 - val_loss: 0.3564\n",
            "Epoch 67/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3036 - val_loss: 0.3563\n",
            "Epoch 68/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3035 - val_loss: 0.3560\n",
            "Epoch 69/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3034 - val_loss: 0.3561\n",
            "Epoch 70/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3033 - val_loss: 0.3559\n",
            "Epoch 71/300\n",
            "6100/6100 [==============================] - 14s 2ms/step - loss: 0.3032 - val_loss: 0.3557\n",
            "Epoch 72/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3030 - val_loss: 0.3554\n",
            "Epoch 73/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3029 - val_loss: 0.3554\n",
            "Epoch 74/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3028 - val_loss: 0.3552\n",
            "Epoch 75/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3027 - val_loss: 0.3551\n",
            "Epoch 76/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3026 - val_loss: 0.3550\n",
            "Epoch 77/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3025 - val_loss: 0.3550\n",
            "Epoch 78/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3024 - val_loss: 0.3548\n",
            "Epoch 79/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3023 - val_loss: 0.3548\n",
            "Epoch 80/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3023 - val_loss: 0.3546\n",
            "Epoch 81/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3022 - val_loss: 0.3547\n",
            "Epoch 82/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3021 - val_loss: 0.3545\n",
            "Epoch 83/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3021 - val_loss: 0.3544\n",
            "Epoch 84/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3020 - val_loss: 0.3544\n",
            "Epoch 85/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3019 - val_loss: 0.3543\n",
            "Epoch 86/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3018 - val_loss: 0.3543\n",
            "Epoch 87/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3017 - val_loss: 0.3541\n",
            "Epoch 88/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3016 - val_loss: 0.3541\n",
            "Epoch 89/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3016 - val_loss: 0.3539\n",
            "Epoch 90/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3015 - val_loss: 0.3539\n",
            "Epoch 91/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3014 - val_loss: 0.3538\n",
            "Epoch 92/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3013 - val_loss: 0.3538\n",
            "Epoch 93/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3013 - val_loss: 0.3537\n",
            "Epoch 94/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3012 - val_loss: 0.3537\n",
            "Epoch 95/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3012 - val_loss: 0.3536\n",
            "Epoch 96/300\n",
            "6100/6100 [==============================] - 14s 2ms/step - loss: 0.3011 - val_loss: 0.3535\n",
            "Epoch 97/300\n",
            "6100/6100 [==============================] - 15s 2ms/step - loss: 0.3011 - val_loss: 0.3536\n",
            "Epoch 98/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3010 - val_loss: 0.3535\n",
            "Epoch 99/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3010 - val_loss: 0.3536\n",
            "Epoch 100/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3009 - val_loss: 0.3534\n",
            "Epoch 101/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3009 - val_loss: 0.3534\n",
            "Epoch 102/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3008 - val_loss: 0.3534\n",
            "Epoch 103/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3008 - val_loss: 0.3534\n",
            "Epoch 104/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3007 - val_loss: 0.3532\n",
            "Epoch 105/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3007 - val_loss: 0.3532\n",
            "Epoch 106/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3006 - val_loss: 0.3533\n",
            "Epoch 107/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3006 - val_loss: 0.3530\n",
            "Epoch 108/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3005 - val_loss: 0.3533\n",
            "Epoch 109/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3005 - val_loss: 0.3532\n",
            "Epoch 110/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3005 - val_loss: 0.3531\n",
            "Epoch 111/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3004 - val_loss: 0.3529\n",
            "Epoch 112/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3004 - val_loss: 0.3529\n",
            "Epoch 113/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3003 - val_loss: 0.3527\n",
            "Epoch 114/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3003 - val_loss: 0.3527\n",
            "Epoch 115/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3002 - val_loss: 0.3526\n",
            "Epoch 116/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3002 - val_loss: 0.3527\n",
            "Epoch 117/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3001 - val_loss: 0.3525\n",
            "Epoch 118/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3001 - val_loss: 0.3525\n",
            "Epoch 119/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3000 - val_loss: 0.3524\n",
            "Epoch 120/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.3000 - val_loss: 0.3526\n",
            "Epoch 121/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.3000 - val_loss: 0.3524\n",
            "Epoch 122/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2999 - val_loss: 0.3524\n",
            "Epoch 123/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2999 - val_loss: 0.3523\n",
            "Epoch 124/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2998 - val_loss: 0.3522\n",
            "Epoch 125/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2998 - val_loss: 0.3523\n",
            "Epoch 126/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2998 - val_loss: 0.3522\n",
            "Epoch 127/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2997 - val_loss: 0.3519\n",
            "Epoch 128/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2997 - val_loss: 0.3521\n",
            "Epoch 129/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2997 - val_loss: 0.3520\n",
            "Epoch 130/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2996 - val_loss: 0.3521\n",
            "Epoch 131/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2996 - val_loss: 0.3519\n",
            "Epoch 132/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2996 - val_loss: 0.3522\n",
            "Epoch 133/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2996 - val_loss: 0.3520\n",
            "Epoch 134/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2995 - val_loss: 0.3520\n",
            "Epoch 135/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2995 - val_loss: 0.3519\n",
            "Epoch 136/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2995 - val_loss: 0.3518\n",
            "Epoch 137/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2995 - val_loss: 0.3519\n",
            "Epoch 138/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2994 - val_loss: 0.3518\n",
            "Epoch 139/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2994 - val_loss: 0.3516\n",
            "Epoch 140/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2994 - val_loss: 0.3517\n",
            "Epoch 141/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2994 - val_loss: 0.3518\n",
            "Epoch 142/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2994 - val_loss: 0.3517\n",
            "Epoch 143/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2993 - val_loss: 0.3519\n",
            "Epoch 144/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2993 - val_loss: 0.3518\n",
            "Epoch 145/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2993 - val_loss: 0.3517\n",
            "Epoch 146/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2993 - val_loss: 0.3518\n",
            "Epoch 147/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2993 - val_loss: 0.3517\n",
            "Epoch 148/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2993 - val_loss: 0.3518\n",
            "Epoch 149/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2992 - val_loss: 0.3516\n",
            "Epoch 150/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2992 - val_loss: 0.3516\n",
            "Epoch 151/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2992 - val_loss: 0.3518\n",
            "Epoch 152/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2992 - val_loss: 0.3517\n",
            "Epoch 153/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2992 - val_loss: 0.3515\n",
            "Epoch 154/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2992 - val_loss: 0.3515\n",
            "Epoch 155/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2991 - val_loss: 0.3516\n",
            "Epoch 156/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2991 - val_loss: 0.3515\n",
            "Epoch 157/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2991 - val_loss: 0.3514\n",
            "Epoch 158/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2991 - val_loss: 0.3516\n",
            "Epoch 159/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2991 - val_loss: 0.3514\n",
            "Epoch 160/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2991 - val_loss: 0.3515\n",
            "Epoch 161/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2991 - val_loss: 0.3516\n",
            "Epoch 162/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2991 - val_loss: 0.3515\n",
            "Epoch 163/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3514\n",
            "Epoch 164/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3517\n",
            "Epoch 165/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3516\n",
            "Epoch 166/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3514\n",
            "Epoch 167/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3514\n",
            "Epoch 168/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3514\n",
            "Epoch 169/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2990 - val_loss: 0.3515\n",
            "Epoch 170/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3513\n",
            "Epoch 171/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3515\n",
            "Epoch 172/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3513\n",
            "Epoch 173/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2989 - val_loss: 0.3513\n",
            "Epoch 174/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3514\n",
            "Epoch 175/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3515\n",
            "Epoch 176/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3513\n",
            "Epoch 177/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3515\n",
            "Epoch 178/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3516\n",
            "Epoch 179/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3514\n",
            "Epoch 180/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2989 - val_loss: 0.3514\n",
            "Epoch 181/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3515\n",
            "Epoch 182/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3514\n",
            "Epoch 183/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 184/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3514\n",
            "Epoch 185/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 186/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 187/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 188/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 189/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3515\n",
            "Epoch 190/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3512\n",
            "Epoch 191/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3513\n",
            "Epoch 192/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2988 - val_loss: 0.3515\n",
            "Epoch 193/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3514\n",
            "Epoch 194/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 195/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3512\n",
            "Epoch 196/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3514\n",
            "Epoch 197/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 198/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 199/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2987 - val_loss: 0.3515\n",
            "Epoch 200/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3514\n",
            "Epoch 201/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 202/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 203/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3515\n",
            "Epoch 204/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3514\n",
            "Epoch 205/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2987 - val_loss: 0.3513\n",
            "Epoch 206/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2987 - val_loss: 0.3514\n",
            "Epoch 207/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2987 - val_loss: 0.3515\n",
            "Epoch 208/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 209/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 210/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 211/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2986 - val_loss: 0.3512\n",
            "Epoch 212/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 213/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3514\n",
            "Epoch 214/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 215/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3512\n",
            "Epoch 216/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3512\n",
            "Epoch 217/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3514\n",
            "Epoch 218/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 219/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3512\n",
            "Epoch 220/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3514\n",
            "Epoch 221/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2986 - val_loss: 0.3513\n",
            "Epoch 222/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 223/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 224/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3512\n",
            "Epoch 225/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 226/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 227/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 228/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 229/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3515\n",
            "Epoch 230/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3512\n",
            "Epoch 231/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 232/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3511\n",
            "Epoch 233/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3514\n",
            "Epoch 234/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 235/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 236/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3515\n",
            "Epoch 237/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3513\n",
            "Epoch 238/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3512\n",
            "Epoch 239/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2985 - val_loss: 0.3512\n",
            "Epoch 240/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3514\n",
            "Epoch 241/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 242/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 243/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 244/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3514\n",
            "Epoch 245/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3511\n",
            "Epoch 246/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 247/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3511\n",
            "Epoch 248/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 249/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3515\n",
            "Epoch 250/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 251/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3511\n",
            "Epoch 252/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 253/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 254/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 255/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 256/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3514\n",
            "Epoch 257/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3512\n",
            "Epoch 258/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3516\n",
            "Epoch 259/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 260/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3515\n",
            "Epoch 261/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 262/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2984 - val_loss: 0.3513\n",
            "Epoch 263/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2984 - val_loss: 0.3514\n",
            "Epoch 264/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 265/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 266/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 267/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 268/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 269/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3516\n",
            "Epoch 270/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 271/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 272/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 273/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 274/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 275/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 276/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 277/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 278/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 279/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 280/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3514\n",
            "Epoch 281/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 282/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 283/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3511\n",
            "Epoch 284/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 285/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 286/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 287/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 288/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 289/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 290/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 291/300\n",
            "6100/6100 [==============================] - 12s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 292/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3512\n",
            "Epoch 293/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2982 - val_loss: 0.3514\n",
            "Epoch 294/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3513\n",
            "Epoch 295/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2983 - val_loss: 0.3515\n",
            "Epoch 296/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2982 - val_loss: 0.3512\n",
            "Epoch 297/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2982 - val_loss: 0.3513\n",
            "Epoch 298/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2982 - val_loss: 0.3513\n",
            "Epoch 299/300\n",
            "6100/6100 [==============================] - 13s 2ms/step - loss: 0.2982 - val_loss: 0.3513\n",
            "Epoch 300/300\n",
            "6100/6100 [==============================] - 14s 2ms/step - loss: 0.2982 - val_loss: 0.3512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f7ea0b96940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpGU86pd9eVA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYOvjfYcUOYQ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "vector_vae = encoder.predict(np.array(train), batch_size = batch_size)\n",
        "pd.DataFrame(vector_vae).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/vector_vae.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQXV88nIDSdi"
      },
      "source": [
        "vector_vae = pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/vector_vae.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNyReoZfWka"
      },
      "source": [
        "#there are only 2 points in the class with label 0, it can not be applied to ADASYN\n",
        "#just simply remove it \n",
        "vector_vae = np.array(vector_vae)\n",
        "index = []\n",
        "for i in range(np.size(label_new, 0)):\n",
        "  if label_new[i] == 0:\n",
        "    index.append(i)\n",
        "if len(index) != 0:\n",
        "  vector_vae = np.delete(vector_vae, index, axis=0)\n",
        "  label_new = np.delete(label_new, index, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBsYehjWg7nZ",
        "outputId": "2ddcfb71-bcc4-46b7-cfd1-a8ddda656a4f"
      },
      "source": [
        "from imblearn.over_sampling import ADASYN \n",
        "\n",
        "ada = ADASYN(random_state=42)\n",
        "vector_vae_balanced, label_new_balanced = ada.fit_resample(vector_vae, label_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fz_bO1QUWco"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(vector_vae_balanced).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/vector_vae_balanced.csv', index=False)\n",
        "pd.DataFrame(label_new_balanced).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/label_new_balanced.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yme7ib04Ji9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxn1wjSzUZWg"
      },
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n",
        "        super(Net, self).__init__()\n",
        "        self.droprate = 0.95\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_1), nn.ReLU(True))\n",
        "        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.Dropout(p=self.droprate), nn.BatchNorm1d(n_hidden_2), nn.ReLU(True))\n",
        "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim), nn.Dropout(p=self.droprate))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/vector_vae_balanced.csv').values).float()\n",
        "        self.labels = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/label_new_balanced.csv').values).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx,:]\n",
        "        labels = self.labels[idx]\n",
        "        return sample, labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tDOq8a4UcAv"
      },
      "source": [
        "def process(X_train, X_test, y_train, y_test):\n",
        "  scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  X_train = np.array(X_train)\n",
        "  X_test = np.array(X_test)\n",
        "  Y_train = np.array(y_train)\n",
        "  Y_test = np.array(y_test)\n",
        "  X_train = torch.from_numpy(X_train).float()\n",
        "  X_test = torch.from_numpy(X_test).float()\n",
        "  Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
        "  Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
        "  train_dataset = []\n",
        "  test_dataset = []\n",
        "  for i in range(len(X_train)):\n",
        "    train_dataset.append((X_train[i],Y_train[i]))\n",
        "  for i in range(len(X_test)):\n",
        "    test_dataset.append((X_test[i],Y_test[i]))\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "#do not standardize the one hot features\n",
        "def process_sentiment(X_train, X_test, y_train, y_test):\n",
        "  sentiment_train = X_train[:,len(X_train[0])-5:len(X_train[0])]\n",
        "  sentiment_test = X_test[:,len(X_test[0])-5:len(X_test[0])]\n",
        "  X_train = X_train[:,:len(X_train[0])-5]\n",
        "  X_test = X_test[:,:len(X_test[0])-5]\n",
        "  scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "  X_train = scaler.transform(X_train)\n",
        "  X_test = scaler.transform(X_test)\n",
        "  X_train = np.hstack((X_train, sentiment_train))\n",
        "  X_test = np.hstack((X_test, sentiment_test))\n",
        "  Y_train = np.array(y_train)\n",
        "  Y_test = np.array(y_test)\n",
        "  X_train = torch.from_numpy(X_train).float()\n",
        "  X_test = torch.from_numpy(X_test).float()\n",
        "  Y_train = torch.from_numpy(Y_train).squeeze().to(torch.int64)\n",
        "  Y_test = torch.from_numpy(Y_test).squeeze().to(torch.int64)\n",
        "  train_dataset = []\n",
        "  test_dataset = []\n",
        "  for i in range(len(X_train)):\n",
        "    train_dataset.append((X_train[i],Y_train[i]))\n",
        "  for i in range(len(X_test)):\n",
        "    test_dataset.append((X_test[i],Y_test[i]))\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, test_loader\n",
        "\n",
        "def classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len_test):\n",
        "  model = Net(input_dim, 100, 30, 4)\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  epoch = 0\n",
        "  train_loss_list = []\n",
        "  loss_list = []\n",
        "  accuracy_list = []\n",
        "  precision_list = []\n",
        "  recall_list = []\n",
        "  F1_list = []\n",
        "  F1_list_train = []\n",
        "\n",
        "  for epoch2 in range(0, 0 + totEpoch):\n",
        "      # model.eval()\n",
        "      num_TP = [0]*num_class\n",
        "      num_FP = [0]*num_class\n",
        "      num_FN = [0]*num_class\n",
        "      eval_loss = 0\n",
        "      eval_acc = 0\n",
        "      eval_TP = [0]*num_class\n",
        "      eval_FP = [0]*num_class\n",
        "      eval_FN = [0]*num_class\n",
        "      precision = [0]*num_class\n",
        "      recall = [0]*num_class\n",
        "      F1 = [0]*num_class\n",
        "      for data in train_loader:\n",
        "          img, label = data\n",
        "          if torch.cuda.is_available():\n",
        "              img = img.cuda()\n",
        "              label = label.cuda()\n",
        "          else:\n",
        "              img = Variable(img)\n",
        "              label = Variable(label)\n",
        "          out = model(img)\n",
        "          loss = criterion(out, label)\n",
        "          print_loss = loss.data.item()\n",
        "          _, pred = torch.max(out, 1)\n",
        "          num_correct = (pred == label).sum()\n",
        "\n",
        "          for i in range(num_class):\n",
        "            num_TP[i] = (((pred == i) & (label == i))).sum()\n",
        "            num_FP[i] = (((pred == i) & (label != i))).sum()\n",
        "            num_FN[i] = (((pred != i) & (label == i))).sum()\n",
        "            eval_TP[i] += num_TP[i].item()\n",
        "            eval_FP[i] += num_FP[i].item()\n",
        "            eval_FN[i] += num_FN[i].item()\n",
        "          eval_acc += num_correct.item()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch+=1\n",
        "\n",
        "      for i in range(num_class):\n",
        "        if (eval_TP[i] + eval_FP[i]) == 0:\n",
        "          precision[i] = 1\n",
        "        else:\n",
        "          precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
        "        if (eval_TP[i] + eval_FN[i]) == 0:\n",
        "          recall[i] = 1\n",
        "        else:\n",
        "          recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
        "        if (precision[i]+recall[i]) == 0:\n",
        "          F1[i] = 0\n",
        "        else:\n",
        "          F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
        "      print('Train F1: {:.6f}'.format(\n",
        "          sum(F1)/num_class,\n",
        "      ))\n",
        "      F1_list_train.append(sum(F1)/num_class)\n",
        "\n",
        "      model.eval()\n",
        "      print('epoch: {}, loss: {:.4}'.format(epoch2, loss.data.item()))\n",
        "      train_loss_list.append(loss.data.item())\n",
        "\n",
        "      # model.eval()\n",
        "      num_TP = [0]*num_class\n",
        "      num_FP = [0]*num_class\n",
        "      num_FN = [0]*num_class\n",
        "      eval_loss = 0\n",
        "      eval_acc = 0\n",
        "      eval_TP = [0]*num_class\n",
        "      eval_FP = [0]*num_class\n",
        "      eval_FN = [0]*num_class\n",
        "      precision = [0]*num_class\n",
        "      recall = [0]*num_class\n",
        "      F1 = [0]*num_class\n",
        "      for data in test_loader:\n",
        "          img, label = data\n",
        "          # img = img.view(img.size(0), -1)\n",
        "          if torch.cuda.is_available():\n",
        "              img = img.cuda()\n",
        "              label = label.cuda()\n",
        "\n",
        "          out = model(img)\n",
        "          loss = criterion(out, label)\n",
        "          eval_loss += loss.data.item()*label.size(0)\n",
        "          _, pred = torch.max(out, 1)\n",
        "          num_correct = (pred == label).sum()\n",
        "          for i in range(num_class):\n",
        "            num_TP[i] = (((pred == i) & (label == i))).sum()\n",
        "            num_FP[i] = (((pred == i) & (label != i))).sum()\n",
        "            num_FN[i] = (((pred != i) & (label == i))).sum()\n",
        "            eval_TP[i] += num_TP[i].item()\n",
        "            eval_FP[i] += num_FP[i].item()\n",
        "            eval_FN[i] += num_FN[i].item()\n",
        "          eval_acc += num_correct.item()\n",
        "      for i in range(num_class):\n",
        "        if (eval_TP[i] + eval_FP[i]) == 0:\n",
        "          precision[i] = 1\n",
        "        else:\n",
        "          precision[i] = eval_TP[i] / (eval_TP[i] + eval_FP[i])\n",
        "        if (eval_TP[i] + eval_FN[i]) == 0:\n",
        "          recall[i] = 1\n",
        "        else:\n",
        "          recall[i] = eval_TP[i] / (eval_TP[i] + eval_FN[i])\n",
        "        if (precision[i]+recall[i]) == 0:\n",
        "          F1[i] = 0\n",
        "        else:\n",
        "          F1[i] = 2*precision[i]*recall[i]/(precision[i]+recall[i])\n",
        "      print('Test Loss: {:.6f}, Acc: {:.6f}, Pre: {:.6f}, Rec: {:.6f}, F1: {:.6f}'.format(\n",
        "          eval_loss / (len_test),\n",
        "          eval_acc / (len_test),\n",
        "          sum(precision)/num_class,\n",
        "          sum(recall)/num_class,\n",
        "          sum(F1)/num_class\n",
        "      ))\n",
        "      loss_list.append(loss.data.item())\n",
        "      accuracy_list.append(eval_acc / (len_test))\n",
        "      precision_list.append(sum(precision)/num_class)\n",
        "      recall_list.append(sum(recall)/num_class)\n",
        "      F1_list.append(sum(F1)/num_class)\n",
        "  return [np.array(accuracy_list), np.array(precision_list), np.array(recall_list), np.array(F1_list), np.array(F1_list_train), np.array(loss_list), np.array(train_loss_list)]\n",
        "\n",
        "def train_model(vector_vae, label, model, totEpoch, num_class, random_s):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(vector_vae, label, test_size=0.2, random_state = random_s)\n",
        "  train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
        "  results= classifier(model, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
        "  return results\n",
        "\n",
        "def train_cross_val(input_dim, vector_vae, label, totEpoch, num_class, k):\n",
        "  results = [np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch),np.array([0.0]*totEpoch)]\n",
        "  kf = StratifiedKFold(n_splits=k)\n",
        "  c = 0\n",
        "  var_test, var_train = [], []\n",
        "  F1_train, F1_test = [], []\n",
        "  Loss_train, Loss_test = [], []\n",
        "  for train_index, test_index in kf.split(vector_vae, label):\n",
        "    print('The ', c, ' th fold cross validation:')\n",
        "    X_train = vector_vae[train_index]\n",
        "    y_train = label[train_index]\n",
        "    X_test = vector_vae[test_index]\n",
        "    y_test = label[test_index]\n",
        "    train_loader, test_loader = process_sentiment(X_train, X_test, y_train, y_test)\n",
        "    result_list= classifier(input_dim, train_loader, test_loader, totEpoch, num_class, len(y_test))\n",
        "    var_test.append(result_list[3])\n",
        "    var_train.append(result_list[4])\n",
        "    F1_train.append(result_list[4])\n",
        "    F1_test.append(result_list[3])\n",
        "    Loss_train.append(result_list[6])\n",
        "    Loss_test.append(result_list[5])\n",
        "    for i in range(7):\n",
        "      results[i] += result_list[i]\n",
        "  for i in range(7):\n",
        "    results[i] /= k\n",
        "  var_test = np.array(var_test)\n",
        "  var_train = np.array(var_train)\n",
        "  results.append(np.var(var_train, axis = 0))\n",
        "  results.append(np.var(var_test, axis = 0))\n",
        "  return results, F1_train, F1_test, Loss_train, Loss_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN7G8skgFUR8",
        "outputId": "0fa55c5a-456d-4daf-bd8f-22c45fe58fed"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "learning_rate = 0.002\n",
        "\n",
        "vector_vae = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/vector_vae_balanced.csv').values).float()\n",
        "label = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/label_new_balanced.csv').values).float()\n",
        "\n",
        "totEpoch = 100\n",
        "num_class = 3\n",
        "\n",
        "results, F1_train, F1_test, Loss_train, Loss_test = train_cross_val(100, vector_vae, label, totEpoch, num_class, 5)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(results[0]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/accuracy_list.csv', index=False)\n",
        "pd.DataFrame(results[1]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/precision_list.csv', index=False)\n",
        "pd.DataFrame(results[2]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/recall_list.csv', index=False)\n",
        "pd.DataFrame(results[3]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_list.csv', index=False)\n",
        "pd.DataFrame(results[4]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_list_train.csv', index=False)\n",
        "pd.DataFrame(results[5]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/loss_list.csv', index=False)\n",
        "pd.DataFrame(results[6]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/train_loss_list.csv', index=False)\n",
        "pd.DataFrame(results[7]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/train_var_list.csv', index=False)\n",
        "pd.DataFrame(results[8]).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/test_var_list.csv', index=False)\n",
        "pd.DataFrame(F1_train).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_train.csv', index=False)\n",
        "pd.DataFrame(F1_test).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_test.csv', index=False)\n",
        "pd.DataFrame(Loss_train).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/Loss_train.csv', index=False)\n",
        "pd.DataFrame(Loss_test).to_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/Loss_test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The  0  th fold cross validation:\n",
            "Train F1: 0.064694\n",
            "epoch: 0, loss: 1.591\n",
            "Test Loss: 1.334995, Acc: 0.331461, Pre: 0.777154, Rec: 0.666667, F1: 0.499297\n",
            "Train F1: 0.499135\n",
            "epoch: 1, loss: 1.225\n",
            "Test Loss: 1.220065, Acc: 0.330056, Pre: 0.515799, Rec: 0.662477, F1: 0.502574\n",
            "Train F1: 0.433542\n",
            "epoch: 2, loss: 1.129\n",
            "Test Loss: 1.127392, Acc: 0.343633, Pre: 1.000000, Rec: 0.333333, F1: 0.333333\n",
            "Train F1: 0.384798\n",
            "epoch: 3, loss: 1.099\n",
            "Test Loss: 1.103334, Acc: 0.375936, Pre: 0.601107, Rec: 0.431342, F1: 0.470744\n",
            "Train F1: 0.495445\n",
            "epoch: 4, loss: 1.097\n",
            "Test Loss: 1.090995, Acc: 0.404494, Pre: 0.601429, Rec: 0.514294, F1: 0.540933\n",
            "Train F1: 0.540829\n",
            "epoch: 5, loss: 1.076\n",
            "Test Loss: 1.076885, Acc: 0.402154, Pre: 0.590439, Rec: 0.525709, F1: 0.505343\n",
            "Train F1: 0.547707\n",
            "epoch: 6, loss: 1.044\n",
            "Test Loss: 1.060678, Acc: 0.433989, Pre: 0.606214, Rec: 0.573568, F1: 0.566041\n",
            "Train F1: 0.591178\n",
            "epoch: 7, loss: 1.032\n",
            "Test Loss: 1.045751, Acc: 0.458801, Pre: 0.618450, Rec: 0.604113, F1: 0.604705\n",
            "Train F1: 0.603244\n",
            "epoch: 8, loss: 0.9861\n",
            "Test Loss: 1.037419, Acc: 0.457397, Pre: 0.614409, Rec: 0.615849, F1: 0.612696\n",
            "Train F1: 0.623223\n",
            "epoch: 9, loss: 1.031\n",
            "Test Loss: 1.021680, Acc: 0.475187, Pre: 0.630094, Rec: 0.612700, F1: 0.620856\n",
            "Train F1: 0.629731\n",
            "epoch: 10, loss: 1.067\n",
            "Test Loss: 1.026887, Acc: 0.464888, Pre: 0.628012, Rec: 0.624257, F1: 0.620328\n",
            "Train F1: 0.641106\n",
            "epoch: 11, loss: 1.036\n",
            "Test Loss: 1.019270, Acc: 0.470037, Pre: 0.626618, Rec: 0.647931, F1: 0.629852\n",
            "Train F1: 0.646323\n",
            "epoch: 12, loss: 1.004\n",
            "Test Loss: 1.008090, Acc: 0.486423, Pre: 0.638142, Rec: 0.627576, F1: 0.632066\n",
            "Train F1: 0.654837\n",
            "epoch: 13, loss: 0.9825\n",
            "Test Loss: 1.006876, Acc: 0.495787, Pre: 0.640249, Rec: 0.643958, F1: 0.639650\n",
            "Train F1: 0.669643\n",
            "epoch: 14, loss: 1.06\n",
            "Test Loss: 1.005128, Acc: 0.496723, Pre: 0.643635, Rec: 0.666291, F1: 0.654406\n",
            "Train F1: 0.683204\n",
            "epoch: 15, loss: 0.8633\n",
            "Test Loss: 1.013154, Acc: 0.506086, Pre: 0.658632, Rec: 0.679289, F1: 0.660034\n",
            "Train F1: 0.694942\n",
            "epoch: 16, loss: 0.9965\n",
            "Test Loss: 1.003078, Acc: 0.510300, Pre: 0.654611, Rec: 0.677277, F1: 0.662780\n",
            "Train F1: 0.709033\n",
            "epoch: 17, loss: 0.858\n",
            "Test Loss: 1.004249, Acc: 0.508427, Pre: 0.659847, Rec: 0.668926, F1: 0.662112\n",
            "Train F1: 0.721028\n",
            "epoch: 18, loss: 0.8606\n",
            "Test Loss: 1.001448, Acc: 0.513109, Pre: 0.651853, Rec: 0.684286, F1: 0.666839\n",
            "Train F1: 0.730789\n",
            "epoch: 19, loss: 0.8659\n",
            "Test Loss: 0.973583, Acc: 0.554775, Pre: 0.686445, Rec: 0.695656, F1: 0.690425\n",
            "Train F1: 0.736694\n",
            "epoch: 20, loss: 0.9034\n",
            "Test Loss: 0.980494, Acc: 0.543539, Pre: 0.689265, Rec: 0.679398, F1: 0.682004\n",
            "Train F1: 0.751080\n",
            "epoch: 21, loss: 0.8018\n",
            "Test Loss: 1.024908, Acc: 0.521067, Pre: 0.662636, Rec: 0.712168, F1: 0.683261\n",
            "Train F1: 0.755652\n",
            "epoch: 22, loss: 0.7533\n",
            "Test Loss: 0.996110, Acc: 0.535581, Pre: 0.678485, Rec: 0.691320, F1: 0.683359\n",
            "Train F1: 0.765265\n",
            "epoch: 23, loss: 0.7994\n",
            "Test Loss: 1.003803, Acc: 0.536985, Pre: 0.678547, Rec: 0.700427, F1: 0.687720\n",
            "Train F1: 0.772093\n",
            "epoch: 24, loss: 0.6813\n",
            "Test Loss: 1.027688, Acc: 0.533708, Pre: 0.698463, Rec: 0.682057, F1: 0.674916\n",
            "Train F1: 0.781905\n",
            "epoch: 25, loss: 0.6971\n",
            "Test Loss: 0.988337, Acc: 0.554775, Pre: 0.686523, Rec: 0.691225, F1: 0.686925\n",
            "Train F1: 0.783840\n",
            "epoch: 26, loss: 0.6906\n",
            "Test Loss: 0.998632, Acc: 0.556180, Pre: 0.689124, Rec: 0.693750, F1: 0.690230\n",
            "Train F1: 0.788990\n",
            "epoch: 27, loss: 0.6251\n",
            "Test Loss: 0.999622, Acc: 0.558989, Pre: 0.705039, Rec: 0.676778, F1: 0.684508\n",
            "Train F1: 0.794113\n",
            "epoch: 28, loss: 0.7275\n",
            "Test Loss: 0.994373, Acc: 0.577715, Pre: 0.698093, Rec: 0.721441, F1: 0.708576\n",
            "Train F1: 0.804586\n",
            "epoch: 29, loss: 0.7837\n",
            "Test Loss: 1.006049, Acc: 0.570225, Pre: 0.695225, Rec: 0.710965, F1: 0.702438\n",
            "Train F1: 0.807634\n",
            "epoch: 30, loss: 0.6615\n",
            "Test Loss: 1.015255, Acc: 0.562266, Pre: 0.692095, Rec: 0.697425, F1: 0.694645\n",
            "Train F1: 0.811939\n",
            "epoch: 31, loss: 0.6464\n",
            "Test Loss: 1.034323, Acc: 0.544476, Pre: 0.688744, Rec: 0.714152, F1: 0.692284\n",
            "Train F1: 0.814981\n",
            "epoch: 32, loss: 0.7676\n",
            "Test Loss: 1.004691, Acc: 0.576311, Pre: 0.695202, Rec: 0.722692, F1: 0.708414\n",
            "Train F1: 0.816778\n",
            "epoch: 33, loss: 0.5527\n",
            "Test Loss: 1.003100, Acc: 0.571629, Pre: 0.693215, Rec: 0.723772, F1: 0.706478\n",
            "Train F1: 0.827277\n",
            "epoch: 34, loss: 0.5463\n",
            "Test Loss: 0.999269, Acc: 0.585674, Pre: 0.707197, Rec: 0.697568, F1: 0.702213\n",
            "Train F1: 0.830238\n",
            "epoch: 35, loss: 0.4419\n",
            "Test Loss: 1.012466, Acc: 0.564607, Pre: 0.695011, Rec: 0.724015, F1: 0.705162\n",
            "Train F1: 0.839749\n",
            "epoch: 36, loss: 0.5648\n",
            "Test Loss: 1.022274, Acc: 0.567884, Pre: 0.696147, Rec: 0.717566, F1: 0.704463\n",
            "Train F1: 0.837552\n",
            "epoch: 37, loss: 0.6481\n",
            "Test Loss: 1.026538, Acc: 0.582865, Pre: 0.698143, Rec: 0.743512, F1: 0.719375\n",
            "Train F1: 0.834306\n",
            "epoch: 38, loss: 0.5821\n",
            "Test Loss: 1.040011, Acc: 0.583333, Pre: 0.707413, Rec: 0.745581, F1: 0.723224\n",
            "Train F1: 0.845296\n",
            "epoch: 39, loss: 0.6136\n",
            "Test Loss: 1.000009, Acc: 0.599251, Pre: 0.714993, Rec: 0.724133, F1: 0.719508\n",
            "Train F1: 0.841557\n",
            "epoch: 40, loss: 0.5522\n",
            "Test Loss: 1.031630, Acc: 0.590356, Pre: 0.702144, Rec: 0.742574, F1: 0.720364\n",
            "Train F1: 0.850875\n",
            "epoch: 41, loss: 0.6807\n",
            "Test Loss: 1.058951, Acc: 0.578184, Pre: 0.694971, Rec: 0.758768, F1: 0.724175\n",
            "Train F1: 0.853607\n",
            "epoch: 42, loss: 0.5017\n",
            "Test Loss: 0.993951, Acc: 0.614232, Pre: 0.719368, Rec: 0.746132, F1: 0.732196\n",
            "Train F1: 0.859986\n",
            "epoch: 43, loss: 0.5474\n",
            "Test Loss: 1.070209, Acc: 0.580056, Pre: 0.705286, Rec: 0.723329, F1: 0.712319\n",
            "Train F1: 0.861466\n",
            "epoch: 44, loss: 0.4458\n",
            "Test Loss: 1.025892, Acc: 0.596910, Pre: 0.707153, Rec: 0.747064, F1: 0.726077\n",
            "Train F1: 0.864717\n",
            "epoch: 45, loss: 0.6359\n",
            "Test Loss: 1.056882, Acc: 0.589888, Pre: 0.709455, Rec: 0.730044, F1: 0.718421\n",
            "Train F1: 0.868120\n",
            "epoch: 46, loss: 0.4375\n",
            "Test Loss: 1.027930, Acc: 0.607678, Pre: 0.715695, Rec: 0.742308, F1: 0.728550\n",
            "Train F1: 0.868326\n",
            "epoch: 47, loss: 0.3973\n",
            "Test Loss: 1.074847, Acc: 0.606273, Pre: 0.736065, Rec: 0.718836, F1: 0.722574\n",
            "Train F1: 0.874104\n",
            "epoch: 48, loss: 0.4208\n",
            "Test Loss: 1.054685, Acc: 0.600187, Pre: 0.723665, Rec: 0.738395, F1: 0.726869\n",
            "Train F1: 0.876042\n",
            "epoch: 49, loss: 0.4957\n",
            "Test Loss: 1.109071, Acc: 0.574438, Pre: 0.705852, Rec: 0.738319, F1: 0.716075\n",
            "Train F1: 0.880701\n",
            "epoch: 50, loss: 0.3596\n",
            "Test Loss: 1.042490, Acc: 0.602528, Pre: 0.730954, Rec: 0.729949, F1: 0.727309\n",
            "Train F1: 0.881688\n",
            "epoch: 51, loss: 0.4575\n",
            "Test Loss: 1.033030, Acc: 0.625936, Pre: 0.729537, Rec: 0.755510, F1: 0.741966\n",
            "Train F1: 0.888491\n",
            "epoch: 52, loss: 0.4847\n",
            "Test Loss: 1.092737, Acc: 0.602996, Pre: 0.719684, Rec: 0.746080, F1: 0.729866\n",
            "Train F1: 0.886235\n",
            "epoch: 53, loss: 0.4097\n",
            "Test Loss: 1.053864, Acc: 0.634363, Pre: 0.731928, Rec: 0.764854, F1: 0.747307\n",
            "Train F1: 0.896212\n",
            "epoch: 54, loss: 0.2729\n",
            "Test Loss: 1.151571, Acc: 0.604869, Pre: 0.722807, Rec: 0.735109, F1: 0.725844\n",
            "Train F1: 0.893917\n",
            "epoch: 55, loss: 0.6181\n",
            "Test Loss: 1.125696, Acc: 0.589419, Pre: 0.714303, Rec: 0.716971, F1: 0.711482\n",
            "Train F1: 0.896461\n",
            "epoch: 56, loss: 0.4369\n",
            "Test Loss: 1.074429, Acc: 0.613764, Pre: 0.733553, Rec: 0.724366, F1: 0.727512\n",
            "Train F1: 0.903536\n",
            "epoch: 57, loss: 0.4515\n",
            "Test Loss: 1.107667, Acc: 0.607210, Pre: 0.730000, Rec: 0.755102, F1: 0.737989\n",
            "Train F1: 0.904091\n",
            "epoch: 58, loss: 0.3686\n",
            "Test Loss: 1.076966, Acc: 0.632959, Pre: 0.737601, Rec: 0.758563, F1: 0.747816\n",
            "Train F1: 0.903126\n",
            "epoch: 59, loss: 0.4357\n",
            "Test Loss: 1.093148, Acc: 0.628277, Pre: 0.743649, Rec: 0.742200, F1: 0.738838\n",
            "Train F1: 0.901738\n",
            "epoch: 60, loss: 0.4547\n",
            "Test Loss: 1.189239, Acc: 0.604401, Pre: 0.728007, Rec: 0.728119, F1: 0.719988\n",
            "Train F1: 0.903312\n",
            "epoch: 61, loss: 0.477\n",
            "Test Loss: 1.091439, Acc: 0.625468, Pre: 0.735818, Rec: 0.756271, F1: 0.744445\n",
            "Train F1: 0.910025\n",
            "epoch: 62, loss: 0.2793\n",
            "Test Loss: 1.180511, Acc: 0.604869, Pre: 0.721063, Rec: 0.776235, F1: 0.744563\n",
            "Train F1: 0.910662\n",
            "epoch: 63, loss: 0.2925\n",
            "Test Loss: 1.064823, Acc: 0.645131, Pre: 0.743076, Rec: 0.765596, F1: 0.754032\n",
            "Train F1: 0.920278\n",
            "epoch: 64, loss: 0.2398\n",
            "Test Loss: 1.143715, Acc: 0.616573, Pre: 0.728012, Rec: 0.756637, F1: 0.739820\n",
            "Train F1: 0.915739\n",
            "epoch: 65, loss: 0.41\n",
            "Test Loss: 1.132683, Acc: 0.636704, Pre: 0.733431, Rec: 0.758382, F1: 0.745493\n",
            "Train F1: 0.923205\n",
            "epoch: 66, loss: 0.3703\n",
            "Test Loss: 1.142625, Acc: 0.633895, Pre: 0.737728, Rec: 0.767166, F1: 0.750481\n",
            "Train F1: 0.918858\n",
            "epoch: 67, loss: 0.3351\n",
            "Test Loss: 1.155796, Acc: 0.637172, Pre: 0.754286, Rec: 0.731447, F1: 0.740715\n",
            "Train F1: 0.924047\n",
            "epoch: 68, loss: 0.3036\n",
            "Test Loss: 1.176001, Acc: 0.625000, Pre: 0.735173, Rec: 0.756148, F1: 0.743192\n",
            "Train F1: 0.925337\n",
            "epoch: 69, loss: 0.2974\n",
            "Test Loss: 1.235872, Acc: 0.617978, Pre: 0.725432, Rec: 0.754283, F1: 0.737399\n",
            "Train F1: 0.928378\n",
            "epoch: 70, loss: 0.2509\n",
            "Test Loss: 1.191709, Acc: 0.632491, Pre: 0.747564, Rec: 0.743840, F1: 0.743242\n",
            "Train F1: 0.929043\n",
            "epoch: 71, loss: 0.2441\n",
            "Test Loss: 1.191375, Acc: 0.636704, Pre: 0.747124, Rec: 0.732212, F1: 0.739327\n",
            "Train F1: 0.927036\n",
            "epoch: 72, loss: 0.2549\n",
            "Test Loss: 1.159773, Acc: 0.655431, Pre: 0.759605, Rec: 0.749936, F1: 0.754707\n",
            "Train F1: 0.932459\n",
            "epoch: 73, loss: 0.2236\n",
            "Test Loss: 1.189290, Acc: 0.661985, Pre: 0.754171, Rec: 0.768159, F1: 0.760626\n",
            "Train F1: 0.933550\n",
            "epoch: 74, loss: 0.295\n",
            "Test Loss: 1.190878, Acc: 0.656835, Pre: 0.756421, Rec: 0.754145, F1: 0.755272\n",
            "Train F1: 0.937278\n",
            "epoch: 75, loss: 0.3237\n",
            "Test Loss: 1.215727, Acc: 0.647472, Pre: 0.754524, Rec: 0.763880, F1: 0.757758\n",
            "Train F1: 0.938664\n",
            "epoch: 76, loss: 0.2276\n",
            "Test Loss: 1.206322, Acc: 0.655899, Pre: 0.757526, Rec: 0.762995, F1: 0.759398\n",
            "Train F1: 0.943270\n",
            "epoch: 77, loss: 0.2445\n",
            "Test Loss: 1.213631, Acc: 0.665262, Pre: 0.766951, Rec: 0.750973, F1: 0.758647\n",
            "Train F1: 0.939608\n",
            "epoch: 78, loss: 0.3184\n",
            "Test Loss: 1.343545, Acc: 0.624064, Pre: 0.732190, Rec: 0.786631, F1: 0.755564\n",
            "Train F1: 0.943009\n",
            "epoch: 79, loss: 0.342\n",
            "Test Loss: 1.309830, Acc: 0.629682, Pre: 0.741917, Rec: 0.757708, F1: 0.743319\n",
            "Train F1: 0.941637\n",
            "epoch: 80, loss: 0.2681\n",
            "Test Loss: 1.266990, Acc: 0.654494, Pre: 0.751132, Rec: 0.768098, F1: 0.758158\n",
            "Train F1: 0.941897\n",
            "epoch: 81, loss: 0.2185\n",
            "Test Loss: 1.353289, Acc: 0.627809, Pre: 0.744212, Rec: 0.757803, F1: 0.745285\n",
            "Train F1: 0.941399\n",
            "epoch: 82, loss: 0.2659\n",
            "Test Loss: 1.283502, Acc: 0.653090, Pre: 0.751692, Rec: 0.760884, F1: 0.754401\n",
            "Train F1: 0.941685\n",
            "epoch: 83, loss: 0.1659\n",
            "Test Loss: 1.324071, Acc: 0.649813, Pre: 0.743515, Rec: 0.776116, F1: 0.759187\n",
            "Train F1: 0.944837\n",
            "epoch: 84, loss: 0.2294\n",
            "Test Loss: 1.358894, Acc: 0.651685, Pre: 0.751854, Rec: 0.771461, F1: 0.760204\n",
            "Train F1: 0.948260\n",
            "epoch: 85, loss: 0.1852\n",
            "Test Loss: 1.318826, Acc: 0.664326, Pre: 0.763770, Rec: 0.776264, F1: 0.768970\n",
            "Train F1: 0.951625\n",
            "epoch: 86, loss: 0.1863\n",
            "Test Loss: 1.349707, Acc: 0.672285, Pre: 0.761947, Rec: 0.776662, F1: 0.768946\n",
            "Train F1: 0.954268\n",
            "epoch: 87, loss: 0.2031\n",
            "Test Loss: 1.378027, Acc: 0.650749, Pre: 0.756178, Rec: 0.751910, F1: 0.752392\n",
            "Train F1: 0.948232\n",
            "epoch: 88, loss: 0.2307\n",
            "Test Loss: 1.349593, Acc: 0.663390, Pre: 0.761581, Rec: 0.757536, F1: 0.759547\n",
            "Train F1: 0.945969\n",
            "epoch: 89, loss: 0.1912\n",
            "Test Loss: 1.422730, Acc: 0.646067, Pre: 0.744512, Rec: 0.785294, F1: 0.762606\n",
            "Train F1: 0.949007\n",
            "epoch: 90, loss: 0.285\n",
            "Test Loss: 1.386172, Acc: 0.662453, Pre: 0.759697, Rec: 0.773334, F1: 0.765028\n",
            "Train F1: 0.956666\n",
            "epoch: 91, loss: 0.164\n",
            "Test Loss: 1.369569, Acc: 0.661049, Pre: 0.754506, Rec: 0.781509, F1: 0.766653\n",
            "Train F1: 0.961530\n",
            "epoch: 92, loss: 0.2877\n",
            "Test Loss: 1.495884, Acc: 0.626404, Pre: 0.738483, Rec: 0.768651, F1: 0.746629\n",
            "Train F1: 0.958218\n",
            "epoch: 93, loss: 0.1953\n",
            "Test Loss: 1.397517, Acc: 0.661985, Pre: 0.761640, Rec: 0.750783, F1: 0.756080\n",
            "Train F1: 0.955479\n",
            "epoch: 94, loss: 0.1063\n",
            "Test Loss: 1.418493, Acc: 0.663390, Pre: 0.761742, Rec: 0.785585, F1: 0.770548\n",
            "Train F1: 0.958839\n",
            "epoch: 95, loss: 0.19\n",
            "Test Loss: 1.400061, Acc: 0.672285, Pre: 0.762641, Rec: 0.789693, F1: 0.775335\n",
            "Train F1: 0.960940\n",
            "epoch: 96, loss: 0.2544\n",
            "Test Loss: 1.466114, Acc: 0.657772, Pre: 0.757641, Rec: 0.758682, F1: 0.757541\n",
            "Train F1: 0.948931\n",
            "epoch: 97, loss: 0.2814\n",
            "Test Loss: 1.458627, Acc: 0.666667, Pre: 0.757875, Rec: 0.773733, F1: 0.765576\n",
            "Train F1: 0.959880\n",
            "epoch: 98, loss: 0.2333\n",
            "Test Loss: 1.466631, Acc: 0.669944, Pre: 0.764720, Rec: 0.762187, F1: 0.763280\n",
            "Train F1: 0.963377\n",
            "epoch: 99, loss: 0.1791\n",
            "Test Loss: 1.443931, Acc: 0.673221, Pre: 0.756325, Rec: 0.785071, F1: 0.770205\n",
            "The  0  th fold cross validation:\n",
            "Train F1: 0.057818\n",
            "epoch: 0, loss: 1.352\n",
            "Test Loss: 1.329103, Acc: 0.329742, Pre: 0.557752, Rec: 0.667025, F1: 0.592355\n",
            "Train F1: 0.608569\n",
            "epoch: 1, loss: 1.21\n",
            "Test Loss: 1.207807, Acc: 0.305386, Pre: 0.478276, Rec: 0.646285, F1: 0.509329\n",
            "Train F1: 0.574357\n",
            "epoch: 2, loss: 1.126\n",
            "Test Loss: 1.124238, Acc: 0.327869, Pre: 0.536895, Rec: 0.540115, F1: 0.538388\n",
            "Train F1: 0.568716\n",
            "epoch: 3, loss: 1.094\n",
            "Test Loss: 1.106436, Acc: 0.324122, Pre: 0.627431, Rec: 0.414436, F1: 0.426247\n",
            "Train F1: 0.549448\n",
            "epoch: 4, loss: 1.071\n",
            "Test Loss: 1.101175, Acc: 0.353162, Pre: 0.557486, Rec: 0.553025, F1: 0.501191\n",
            "Train F1: 0.592475\n",
            "epoch: 5, loss: 1.07\n",
            "Test Loss: 1.096651, Acc: 0.366745, Pre: 0.546134, Rec: 0.502651, F1: 0.521214\n",
            "Train F1: 0.626683\n",
            "epoch: 6, loss: 1.072\n",
            "Test Loss: 1.094459, Acc: 0.377986, Pre: 0.560455, Rec: 0.511602, F1: 0.525920\n",
            "Train F1: 0.634297\n",
            "epoch: 7, loss: 1.003\n",
            "Test Loss: 1.098946, Acc: 0.385012, Pre: 0.560385, Rec: 0.514898, F1: 0.528553\n",
            "Train F1: 0.642054\n",
            "epoch: 8, loss: 1.034\n",
            "Test Loss: 1.103916, Acc: 0.402810, Pre: 0.570881, Rec: 0.528612, F1: 0.546573\n",
            "Train F1: 0.645612\n",
            "epoch: 9, loss: 0.9493\n",
            "Test Loss: 1.098636, Acc: 0.402342, Pre: 0.573433, Rec: 0.524523, F1: 0.541745\n",
            "Train F1: 0.652446\n",
            "epoch: 10, loss: 1.028\n",
            "Test Loss: 1.104328, Acc: 0.408431, Pre: 0.581481, Rec: 0.528574, F1: 0.549883\n",
            "Train F1: 0.661705\n",
            "epoch: 11, loss: 0.986\n",
            "Test Loss: 1.091195, Acc: 0.412178, Pre: 0.585927, Rec: 0.517518, F1: 0.542350\n",
            "Train F1: 0.671486\n",
            "epoch: 12, loss: 0.9628\n",
            "Test Loss: 1.107308, Acc: 0.414988, Pre: 0.589015, Rec: 0.553750, F1: 0.570076\n",
            "Train F1: 0.688492\n",
            "epoch: 13, loss: 0.9312\n",
            "Test Loss: 1.095466, Acc: 0.434660, Pre: 0.597687, Rec: 0.563270, F1: 0.579227\n",
            "Train F1: 0.697911\n",
            "epoch: 14, loss: 0.9882\n",
            "Test Loss: 1.072693, Acc: 0.445433, Pre: 0.621839, Rec: 0.598344, F1: 0.606010\n",
            "Train F1: 0.713025\n",
            "epoch: 15, loss: 0.9279\n",
            "Test Loss: 1.117752, Acc: 0.431382, Pre: 0.608983, Rec: 0.604744, F1: 0.606843\n",
            "Train F1: 0.727275\n",
            "epoch: 16, loss: 0.8901\n",
            "Test Loss: 1.130048, Acc: 0.427635, Pre: 0.611119, Rec: 0.580848, F1: 0.590796\n",
            "Train F1: 0.726467\n",
            "epoch: 17, loss: 0.7832\n",
            "Test Loss: 1.062767, Acc: 0.477283, Pre: 0.662832, Rec: 0.581894, F1: 0.614447\n",
            "Train F1: 0.739295\n",
            "epoch: 18, loss: 0.9258\n",
            "Test Loss: 1.100751, Acc: 0.469321, Pre: 0.629342, Rec: 0.610513, F1: 0.618369\n",
            "Train F1: 0.745321\n",
            "epoch: 19, loss: 0.98\n",
            "Test Loss: 1.115155, Acc: 0.469789, Pre: 0.634463, Rec: 0.616947, F1: 0.624939\n",
            "Train F1: 0.755318\n",
            "epoch: 20, loss: 0.8295\n",
            "Test Loss: 1.121971, Acc: 0.462295, Pre: 0.628601, Rec: 0.625725, F1: 0.627152\n",
            "Train F1: 0.754868\n",
            "epoch: 21, loss: 0.8211\n",
            "Test Loss: 1.092518, Acc: 0.487119, Pre: 0.632309, Rec: 0.643910, F1: 0.637942\n",
            "Train F1: 0.768478\n",
            "epoch: 22, loss: 0.854\n",
            "Test Loss: 1.047282, Acc: 0.516628, Pre: 0.673719, Rec: 0.654716, F1: 0.662211\n",
            "Train F1: 0.766558\n",
            "epoch: 23, loss: 0.8775\n",
            "Test Loss: 1.139379, Acc: 0.458080, Pre: 0.633524, Rec: 0.640706, F1: 0.634957\n",
            "Train F1: 0.777446\n",
            "epoch: 24, loss: 0.7794\n",
            "Test Loss: 1.107750, Acc: 0.504450, Pre: 0.647094, Rec: 0.662804, F1: 0.651872\n",
            "Train F1: 0.783720\n",
            "epoch: 25, loss: 0.8319\n",
            "Test Loss: 1.069558, Acc: 0.511007, Pre: 0.670547, Rec: 0.629440, F1: 0.645873\n",
            "Train F1: 0.786696\n",
            "epoch: 26, loss: 0.8543\n",
            "Test Loss: 1.140676, Acc: 0.505386, Pre: 0.648831, Rec: 0.685931, F1: 0.665955\n",
            "Train F1: 0.791680\n",
            "epoch: 27, loss: 0.6489\n",
            "Test Loss: 1.129954, Acc: 0.496019, Pre: 0.654572, Rec: 0.640396, F1: 0.647025\n",
            "Train F1: 0.797049\n",
            "epoch: 28, loss: 0.7771\n",
            "Test Loss: 1.076942, Acc: 0.522248, Pre: 0.671678, Rec: 0.672977, F1: 0.670873\n",
            "Train F1: 0.799722\n",
            "epoch: 29, loss: 0.7114\n",
            "Test Loss: 1.088567, Acc: 0.521311, Pre: 0.673658, Rec: 0.653835, F1: 0.663430\n",
            "Train F1: 0.806709\n",
            "epoch: 30, loss: 0.7288\n",
            "Test Loss: 1.119425, Acc: 0.532084, Pre: 0.690684, Rec: 0.638599, F1: 0.662562\n",
            "Train F1: 0.805208\n",
            "epoch: 31, loss: 0.6714\n",
            "Test Loss: 1.093032, Acc: 0.548478, Pre: 0.684778, Rec: 0.675288, F1: 0.679948\n",
            "Train F1: 0.813228\n",
            "epoch: 32, loss: 0.6557\n",
            "Test Loss: 1.074480, Acc: 0.528806, Pre: 0.671818, Rec: 0.659652, F1: 0.664228\n",
            "Train F1: 0.816303\n",
            "epoch: 33, loss: 0.5535\n",
            "Test Loss: 1.206748, Acc: 0.469789, Pre: 0.661473, Rec: 0.647950, F1: 0.636828\n",
            "Train F1: 0.816102\n",
            "epoch: 34, loss: 0.4965\n",
            "Test Loss: 1.187950, Acc: 0.506792, Pre: 0.659544, Rec: 0.686373, F1: 0.672405\n",
            "Train F1: 0.822880\n",
            "epoch: 35, loss: 0.4803\n",
            "Test Loss: 1.109407, Acc: 0.533958, Pre: 0.680918, Rec: 0.693478, F1: 0.685898\n",
            "Train F1: 0.824786\n",
            "epoch: 36, loss: 0.6383\n",
            "Test Loss: 1.151274, Acc: 0.522248, Pre: 0.685970, Rec: 0.685774, F1: 0.682912\n",
            "Train F1: 0.824698\n",
            "epoch: 37, loss: 0.5991\n",
            "Test Loss: 1.249639, Acc: 0.477752, Pre: 0.644576, Rec: 0.660955, F1: 0.651353\n",
            "Train F1: 0.831110\n",
            "epoch: 38, loss: 0.5844\n",
            "Test Loss: 1.089602, Acc: 0.558314, Pre: 0.686670, Rec: 0.701262, F1: 0.693544\n",
            "Train F1: 0.841823\n",
            "epoch: 39, loss: 0.4025\n",
            "Test Loss: 1.129032, Acc: 0.543326, Pre: 0.698024, Rec: 0.697615, F1: 0.697303\n",
            "Train F1: 0.842020\n",
            "epoch: 40, loss: 0.52\n",
            "Test Loss: 1.118812, Acc: 0.558314, Pre: 0.684419, Rec: 0.708681, F1: 0.696144\n",
            "Train F1: 0.845929\n",
            "epoch: 41, loss: 0.5772\n",
            "Test Loss: 1.199635, Acc: 0.524590, Pre: 0.669543, Rec: 0.668425, F1: 0.668927\n",
            "Train F1: 0.847155\n",
            "epoch: 42, loss: 0.6002\n",
            "Test Loss: 1.118094, Acc: 0.557377, Pre: 0.701677, Rec: 0.691833, F1: 0.696186\n",
            "Train F1: 0.851184\n",
            "epoch: 43, loss: 0.5884\n",
            "Test Loss: 1.122467, Acc: 0.549883, Pre: 0.705261, Rec: 0.672915, F1: 0.688011\n",
            "Train F1: 0.852996\n",
            "epoch: 44, loss: 0.5968\n",
            "Test Loss: 1.233697, Acc: 0.557845, Pre: 0.696005, Rec: 0.646882, F1: 0.666030\n",
            "Train F1: 0.854147\n",
            "epoch: 45, loss: 0.5338\n",
            "Test Loss: 1.162609, Acc: 0.556440, Pre: 0.683410, Rec: 0.716480, F1: 0.698889\n",
            "Train F1: 0.858550\n",
            "epoch: 46, loss: 0.4476\n",
            "Test Loss: 1.122980, Acc: 0.573770, Pre: 0.696513, Rec: 0.725040, F1: 0.710093\n",
            "Train F1: 0.863948\n",
            "epoch: 47, loss: 0.4702\n",
            "Test Loss: 1.160677, Acc: 0.571897, Pre: 0.710276, Rec: 0.684104, F1: 0.696685\n",
            "Train F1: 0.866751\n",
            "epoch: 48, loss: 0.5336\n",
            "Test Loss: 1.196487, Acc: 0.554567, Pre: 0.689100, Rec: 0.705057, F1: 0.696891\n",
            "Train F1: 0.866100\n",
            "epoch: 49, loss: 0.5634\n",
            "Test Loss: 1.211245, Acc: 0.552225, Pre: 0.693282, Rec: 0.704829, F1: 0.698408\n",
            "Train F1: 0.872327\n",
            "epoch: 50, loss: 0.3407\n",
            "Test Loss: 1.162405, Acc: 0.585480, Pre: 0.720290, Rec: 0.697168, F1: 0.706644\n",
            "Train F1: 0.860758\n",
            "epoch: 51, loss: 0.6382\n",
            "Test Loss: 1.165095, Acc: 0.564871, Pre: 0.696285, Rec: 0.718278, F1: 0.706957\n",
            "Train F1: 0.872407\n",
            "epoch: 52, loss: 0.403\n",
            "Test Loss: 1.167253, Acc: 0.571429, Pre: 0.731786, Rec: 0.686154, F1: 0.706547\n",
            "Train F1: 0.876371\n",
            "epoch: 53, loss: 0.573\n",
            "Test Loss: 1.190495, Acc: 0.566276, Pre: 0.701450, Rec: 0.724945, F1: 0.712763\n",
            "Train F1: 0.881339\n",
            "epoch: 54, loss: 0.6309\n",
            "Test Loss: 1.183583, Acc: 0.581265, Pre: 0.707281, Rec: 0.704530, F1: 0.705886\n",
            "Train F1: 0.880320\n",
            "epoch: 55, loss: 0.418\n",
            "Test Loss: 1.176323, Acc: 0.572834, Pre: 0.707029, Rec: 0.736706, F1: 0.720700\n",
            "Train F1: 0.884252\n",
            "epoch: 56, loss: 0.4509\n",
            "Test Loss: 1.146689, Acc: 0.598126, Pre: 0.746206, Rec: 0.706908, F1: 0.723360\n",
            "Train F1: 0.886909\n",
            "epoch: 57, loss: 0.3663\n",
            "Test Loss: 1.161388, Acc: 0.596721, Pre: 0.716936, Rec: 0.736906, F1: 0.726666\n",
            "Train F1: 0.891440\n",
            "epoch: 58, loss: 0.292\n",
            "Test Loss: 1.215926, Acc: 0.579391, Pre: 0.714431, Rec: 0.727827, F1: 0.720993\n",
            "Train F1: 0.893033\n",
            "epoch: 59, loss: 0.4454\n",
            "Test Loss: 1.215509, Acc: 0.588759, Pre: 0.719794, Rec: 0.734627, F1: 0.727051\n",
            "Train F1: 0.896693\n",
            "epoch: 60, loss: 0.4023\n",
            "Test Loss: 1.199622, Acc: 0.592506, Pre: 0.710116, Rec: 0.720165, F1: 0.712574\n",
            "Train F1: 0.899338\n",
            "epoch: 61, loss: 0.2978\n",
            "Test Loss: 1.267885, Acc: 0.584075, Pre: 0.711808, Rec: 0.723195, F1: 0.717403\n",
            "Train F1: 0.897493\n",
            "epoch: 62, loss: 0.4379\n",
            "Test Loss: 1.202575, Acc: 0.606089, Pre: 0.740664, Rec: 0.729634, F1: 0.734876\n",
            "Train F1: 0.898182\n",
            "epoch: 63, loss: 0.3838\n",
            "Test Loss: 1.240983, Acc: 0.599063, Pre: 0.726330, Rec: 0.736340, F1: 0.731156\n",
            "Train F1: 0.903695\n",
            "epoch: 64, loss: 0.259\n",
            "Test Loss: 1.235895, Acc: 0.597658, Pre: 0.719109, Rec: 0.721716, F1: 0.720387\n",
            "Train F1: 0.909896\n",
            "epoch: 65, loss: 0.3104\n",
            "Test Loss: 1.177911, Acc: 0.621546, Pre: 0.742242, Rec: 0.753008, F1: 0.747501\n",
            "Train F1: 0.912368\n",
            "epoch: 66, loss: 0.5536\n",
            "Test Loss: 1.183340, Acc: 0.620141, Pre: 0.740060, Rec: 0.737490, F1: 0.738329\n",
            "Train F1: 0.906694\n",
            "epoch: 67, loss: 0.3871\n",
            "Test Loss: 1.263104, Acc: 0.597658, Pre: 0.721160, Rec: 0.745209, F1: 0.732513\n",
            "Train F1: 0.911584\n",
            "epoch: 68, loss: 0.2684\n",
            "Test Loss: 1.269598, Acc: 0.619672, Pre: 0.732494, Rec: 0.724712, F1: 0.728003\n",
            "Train F1: 0.916700\n",
            "epoch: 69, loss: 0.2495\n",
            "Test Loss: 1.299917, Acc: 0.605621, Pre: 0.724221, Rec: 0.745061, F1: 0.734337\n",
            "Train F1: 0.921725\n",
            "epoch: 70, loss: 0.2557\n",
            "Test Loss: 1.268116, Acc: 0.619672, Pre: 0.727625, Rec: 0.749337, F1: 0.737856\n",
            "Train F1: 0.919975\n",
            "epoch: 71, loss: 0.3137\n",
            "Test Loss: 1.210003, Acc: 0.621077, Pre: 0.729616, Rec: 0.742056, F1: 0.735738\n",
            "Train F1: 0.920762\n",
            "epoch: 72, loss: 0.3688\n",
            "Test Loss: 1.313843, Acc: 0.610773, Pre: 0.732306, Rec: 0.737490, F1: 0.734600\n",
            "Train F1: 0.923832\n",
            "epoch: 73, loss: 0.3449\n",
            "Test Loss: 1.233265, Acc: 0.630913, Pre: 0.755109, Rec: 0.743045, F1: 0.748989\n",
            "Train F1: 0.926590\n",
            "epoch: 74, loss: 0.288\n",
            "Test Loss: 1.252666, Acc: 0.622014, Pre: 0.738776, Rec: 0.738532, F1: 0.738308\n",
            "Train F1: 0.926271\n",
            "epoch: 75, loss: 0.311\n",
            "Test Loss: 1.335029, Acc: 0.623888, Pre: 0.728316, Rec: 0.756456, F1: 0.741768\n",
            "Train F1: 0.925686\n",
            "epoch: 76, loss: 0.2682\n",
            "Test Loss: 1.290750, Acc: 0.623888, Pre: 0.743710, Rec: 0.751990, F1: 0.747660\n",
            "Train F1: 0.923039\n",
            "epoch: 77, loss: 0.4077\n",
            "Test Loss: 1.329584, Acc: 0.631850, Pre: 0.740393, Rec: 0.752775, F1: 0.745818\n",
            "Train F1: 0.923630\n",
            "epoch: 78, loss: 0.261\n",
            "Test Loss: 1.442108, Acc: 0.607026, Pre: 0.724084, Rec: 0.746992, F1: 0.735064\n",
            "Train F1: 0.924372\n",
            "epoch: 79, loss: 0.3557\n",
            "Test Loss: 1.329722, Acc: 0.627635, Pre: 0.756679, Rec: 0.733268, F1: 0.743816\n",
            "Train F1: 0.930049\n",
            "epoch: 80, loss: 0.3362\n",
            "Test Loss: 1.360132, Acc: 0.629977, Pre: 0.735013, Rec: 0.758386, F1: 0.746177\n",
            "Train F1: 0.937522\n",
            "epoch: 81, loss: 0.307\n",
            "Test Loss: 1.380393, Acc: 0.623888, Pre: 0.730733, Rec: 0.748951, F1: 0.739130\n",
            "Train F1: 0.936845\n",
            "epoch: 82, loss: 0.1566\n",
            "Test Loss: 1.465343, Acc: 0.615925, Pre: 0.729724, Rec: 0.766157, F1: 0.746844\n",
            "Train F1: 0.942455\n",
            "epoch: 83, loss: 0.2694\n",
            "Test Loss: 1.410132, Acc: 0.633255, Pre: 0.740345, Rec: 0.734109, F1: 0.736802\n",
            "Train F1: 0.939771\n",
            "epoch: 84, loss: 0.3464\n",
            "Test Loss: 1.454755, Acc: 0.613583, Pre: 0.742154, Rec: 0.748391, F1: 0.743563\n",
            "Train F1: 0.935923\n",
            "epoch: 85, loss: 0.3025\n",
            "Test Loss: 1.406164, Acc: 0.626698, Pre: 0.738805, Rec: 0.763404, F1: 0.750650\n",
            "Train F1: 0.940350\n",
            "epoch: 86, loss: 0.274\n",
            "Test Loss: 1.462509, Acc: 0.637471, Pre: 0.754328, Rec: 0.743339, F1: 0.747779\n",
            "Train F1: 0.931551\n",
            "epoch: 87, loss: 0.2088\n",
            "Test Loss: 1.418520, Acc: 0.635597, Pre: 0.742918, Rec: 0.760460, F1: 0.750404\n",
            "Train F1: 0.944498\n",
            "epoch: 88, loss: 0.3036\n",
            "Test Loss: 1.522240, Acc: 0.616862, Pre: 0.731449, Rec: 0.762096, F1: 0.746180\n",
            "Train F1: 0.941896\n",
            "epoch: 89, loss: 0.1819\n",
            "Test Loss: 1.359367, Acc: 0.661827, Pre: 0.764015, Rec: 0.757397, F1: 0.760329\n",
            "Train F1: 0.949481\n",
            "epoch: 90, loss: 0.2667\n",
            "Test Loss: 1.455892, Acc: 0.644028, Pre: 0.747911, Rec: 0.755975, F1: 0.751619\n",
            "Train F1: 0.951341\n",
            "epoch: 91, loss: 0.2354\n",
            "Test Loss: 1.430405, Acc: 0.634660, Pre: 0.741516, Rec: 0.792983, F1: 0.765724\n",
            "Train F1: 0.947965\n",
            "epoch: 92, loss: 0.1967\n",
            "Test Loss: 1.471733, Acc: 0.646838, Pre: 0.756202, Rec: 0.743377, F1: 0.748210\n",
            "Train F1: 0.951187\n",
            "epoch: 93, loss: 0.142\n",
            "Test Loss: 1.386834, Acc: 0.661358, Pre: 0.768362, Rec: 0.756237, F1: 0.762212\n",
            "Train F1: 0.953084\n",
            "epoch: 94, loss: 0.2417\n",
            "Test Loss: 1.515097, Acc: 0.629977, Pre: 0.741372, Rec: 0.757602, F1: 0.748998\n",
            "Train F1: 0.958283\n",
            "epoch: 95, loss: 0.23\n",
            "Test Loss: 1.576731, Acc: 0.634660, Pre: 0.737636, Rec: 0.772220, F1: 0.754031\n",
            "Train F1: 0.955479\n",
            "epoch: 96, loss: 0.1967\n",
            "Test Loss: 1.504092, Acc: 0.643091, Pre: 0.748793, Rec: 0.778736, F1: 0.763239\n",
            "Train F1: 0.950587\n",
            "epoch: 97, loss: 0.2811\n",
            "Test Loss: 1.606637, Acc: 0.627635, Pre: 0.748118, Rec: 0.767708, F1: 0.757396\n",
            "Train F1: 0.956734\n",
            "epoch: 98, loss: 0.2675\n",
            "Test Loss: 1.620751, Acc: 0.632319, Pre: 0.736719, Rec: 0.783324, F1: 0.757254\n",
            "Train F1: 0.949560\n",
            "epoch: 99, loss: 0.1379\n",
            "Test Loss: 1.593325, Acc: 0.647307, Pre: 0.745266, Rec: 0.753397, F1: 0.746918\n",
            "The  0  th fold cross validation:\n",
            "Train F1: 0.046560\n",
            "epoch: 0, loss: 1.504\n",
            "Test Loss: 1.335127, Acc: 0.331148, Pre: 0.777049, Rec: 0.666667, F1: 0.499179\n",
            "Train F1: 0.499252\n",
            "epoch: 1, loss: 1.219\n",
            "Test Loss: 1.217328, Acc: 0.330679, Pre: 0.443715, Rec: 0.666195, F1: 0.499119\n",
            "Train F1: 0.466303\n",
            "epoch: 2, loss: 1.126\n",
            "Test Loss: 1.130448, Acc: 0.348009, Pre: 0.803922, Rec: 0.340058, F1: 0.346154\n",
            "Train F1: 0.465721\n",
            "epoch: 3, loss: 1.095\n",
            "Test Loss: 1.108774, Acc: 0.347541, Pre: 0.553368, Rec: 0.404818, F1: 0.434021\n",
            "Train F1: 0.536350\n",
            "epoch: 4, loss: 1.089\n",
            "Test Loss: 1.101648, Acc: 0.355504, Pre: 0.577894, Rec: 0.503732, F1: 0.527812\n",
            "Train F1: 0.574908\n",
            "epoch: 5, loss: 1.049\n",
            "Test Loss: 1.102155, Acc: 0.367213, Pre: 0.578463, Rec: 0.531599, F1: 0.551747\n",
            "Train F1: 0.588842\n",
            "epoch: 6, loss: 1.026\n",
            "Test Loss: 1.106589, Acc: 0.386417, Pre: 0.592221, Rec: 0.545118, F1: 0.566312\n",
            "Train F1: 0.607300\n",
            "epoch: 7, loss: 1.038\n",
            "Test Loss: 1.098164, Acc: 0.404215, Pre: 0.602103, Rec: 0.585542, F1: 0.588499\n",
            "Train F1: 0.615909\n",
            "epoch: 8, loss: 0.9829\n",
            "Test Loss: 1.097889, Acc: 0.418735, Pre: 0.617124, Rec: 0.564225, F1: 0.587864\n",
            "Train F1: 0.627360\n",
            "epoch: 9, loss: 0.9638\n",
            "Test Loss: 1.089103, Acc: 0.423888, Pre: 0.615207, Rec: 0.588217, F1: 0.599824\n",
            "Train F1: 0.633489\n",
            "epoch: 10, loss: 0.9724\n",
            "Test Loss: 1.090242, Acc: 0.424356, Pre: 0.614428, Rec: 0.595761, F1: 0.603154\n",
            "Train F1: 0.636599\n",
            "epoch: 11, loss: 1.02\n",
            "Test Loss: 1.098112, Acc: 0.433255, Pre: 0.625336, Rec: 0.577995, F1: 0.599577\n",
            "Train F1: 0.644837\n",
            "epoch: 12, loss: 0.9452\n",
            "Test Loss: 1.088874, Acc: 0.432787, Pre: 0.620604, Rec: 0.583511, F1: 0.600192\n",
            "Train F1: 0.656195\n",
            "epoch: 13, loss: 0.9684\n",
            "Test Loss: 1.076848, Acc: 0.434192, Pre: 0.619393, Rec: 0.598651, F1: 0.607308\n",
            "Train F1: 0.673074\n",
            "epoch: 14, loss: 0.9922\n",
            "Test Loss: 1.079311, Acc: 0.447307, Pre: 0.632720, Rec: 0.606928, F1: 0.616826\n",
            "Train F1: 0.681050\n",
            "epoch: 15, loss: 1.031\n",
            "Test Loss: 1.057614, Acc: 0.456206, Pre: 0.634474, Rec: 0.628324, F1: 0.630988\n",
            "Train F1: 0.698870\n",
            "epoch: 16, loss: 0.8544\n",
            "Test Loss: 1.056350, Acc: 0.476347, Pre: 0.653338, Rec: 0.613128, F1: 0.629821\n",
            "Train F1: 0.705632\n",
            "epoch: 17, loss: 0.8499\n",
            "Test Loss: 1.037911, Acc: 0.489930, Pre: 0.658005, Rec: 0.650692, F1: 0.652822\n",
            "Train F1: 0.716787\n",
            "epoch: 18, loss: 0.8881\n",
            "Test Loss: 1.031527, Acc: 0.494614, Pre: 0.654007, Rec: 0.666401, F1: 0.659100\n",
            "Train F1: 0.722710\n",
            "epoch: 19, loss: 0.9138\n",
            "Test Loss: 0.998604, Acc: 0.529274, Pre: 0.683912, Rec: 0.672707, F1: 0.675662\n",
            "Train F1: 0.734695\n",
            "epoch: 20, loss: 0.8502\n",
            "Test Loss: 1.004195, Acc: 0.531148, Pre: 0.682780, Rec: 0.674121, F1: 0.676826\n",
            "Train F1: 0.733820\n",
            "epoch: 21, loss: 0.7725\n",
            "Test Loss: 1.060141, Acc: 0.509602, Pre: 0.664488, Rec: 0.707633, F1: 0.672118\n",
            "Train F1: 0.746022\n",
            "epoch: 22, loss: 0.8053\n",
            "Test Loss: 1.018230, Acc: 0.526932, Pre: 0.676556, Rec: 0.690130, F1: 0.677692\n",
            "Train F1: 0.754053\n",
            "epoch: 23, loss: 0.7603\n",
            "Test Loss: 1.024503, Acc: 0.526932, Pre: 0.697940, Rec: 0.645815, F1: 0.663212\n",
            "Train F1: 0.756740\n",
            "epoch: 24, loss: 0.8241\n",
            "Test Loss: 0.977618, Acc: 0.557377, Pre: 0.699224, Rec: 0.708818, F1: 0.695655\n",
            "Train F1: 0.763508\n",
            "epoch: 25, loss: 0.7596\n",
            "Test Loss: 1.012382, Acc: 0.552693, Pre: 0.720101, Rec: 0.659550, F1: 0.681864\n",
            "Train F1: 0.762405\n",
            "epoch: 26, loss: 0.858\n",
            "Test Loss: 1.002532, Acc: 0.568150, Pre: 0.703586, Rec: 0.725131, F1: 0.702511\n",
            "Train F1: 0.770309\n",
            "epoch: 27, loss: 0.7709\n",
            "Test Loss: 0.951600, Acc: 0.573770, Pre: 0.710239, Rec: 0.715405, F1: 0.712263\n",
            "Train F1: 0.777312\n",
            "epoch: 28, loss: 0.7495\n",
            "Test Loss: 0.981229, Acc: 0.567681, Pre: 0.714663, Rec: 0.683274, F1: 0.696268\n",
            "Train F1: 0.781200\n",
            "epoch: 29, loss: 0.8822\n",
            "Test Loss: 0.944489, Acc: 0.575176, Pre: 0.715275, Rec: 0.710218, F1: 0.712015\n",
            "Train F1: 0.784635\n",
            "epoch: 30, loss: 0.6986\n",
            "Test Loss: 1.031558, Acc: 0.556440, Pre: 0.715554, Rec: 0.676726, F1: 0.689743\n",
            "Train F1: 0.791640\n",
            "epoch: 31, loss: 0.7536\n",
            "Test Loss: 0.929669, Acc: 0.599063, Pre: 0.726794, Rec: 0.725977, F1: 0.717497\n",
            "Train F1: 0.789811\n",
            "epoch: 32, loss: 0.8519\n",
            "Test Loss: 0.950920, Acc: 0.596721, Pre: 0.737393, Rec: 0.709911, F1: 0.709277\n",
            "Train F1: 0.799307\n",
            "epoch: 33, loss: 0.749\n",
            "Test Loss: 0.943790, Acc: 0.598595, Pre: 0.738142, Rec: 0.708214, F1: 0.717602\n",
            "Train F1: 0.797792\n",
            "epoch: 34, loss: 0.6901\n",
            "Test Loss: 0.907654, Acc: 0.605621, Pre: 0.741011, Rec: 0.700256, F1: 0.718676\n",
            "Train F1: 0.804862\n",
            "epoch: 35, loss: 0.696\n",
            "Test Loss: 0.904592, Acc: 0.619204, Pre: 0.734969, Rec: 0.748145, F1: 0.737706\n",
            "Train F1: 0.810820\n",
            "epoch: 36, loss: 0.7227\n",
            "Test Loss: 0.899029, Acc: 0.615925, Pre: 0.728481, Rec: 0.767378, F1: 0.746622\n",
            "Train F1: 0.811711\n",
            "epoch: 37, loss: 0.7245\n",
            "Test Loss: 0.941549, Acc: 0.602810, Pre: 0.734521, Rec: 0.725544, F1: 0.728186\n",
            "Train F1: 0.816267\n",
            "epoch: 38, loss: 0.6617\n",
            "Test Loss: 0.919570, Acc: 0.628103, Pre: 0.754604, Rec: 0.726884, F1: 0.733846\n",
            "Train F1: 0.819603\n",
            "epoch: 39, loss: 0.6381\n",
            "Test Loss: 0.925817, Acc: 0.617330, Pre: 0.743634, Rec: 0.724848, F1: 0.729927\n",
            "Train F1: 0.822480\n",
            "epoch: 40, loss: 0.5005\n",
            "Test Loss: 0.898092, Acc: 0.616862, Pre: 0.732662, Rec: 0.763391, F1: 0.744227\n",
            "Train F1: 0.823332\n",
            "epoch: 41, loss: 0.5578\n",
            "Test Loss: 0.858731, Acc: 0.646838, Pre: 0.767725, Rec: 0.748083, F1: 0.754635\n",
            "Train F1: 0.829981\n",
            "epoch: 42, loss: 0.5711\n",
            "Test Loss: 0.872863, Acc: 0.626230, Pre: 0.745824, Rec: 0.748722, F1: 0.742156\n",
            "Train F1: 0.835663\n",
            "epoch: 43, loss: 0.5598\n",
            "Test Loss: 0.871699, Acc: 0.626230, Pre: 0.753905, Rec: 0.728500, F1: 0.740557\n",
            "Train F1: 0.842642\n",
            "epoch: 44, loss: 0.6531\n",
            "Test Loss: 0.880666, Acc: 0.636534, Pre: 0.765779, Rec: 0.734472, F1: 0.744200\n",
            "Train F1: 0.838544\n",
            "epoch: 45, loss: 0.6459\n",
            "Test Loss: 0.876116, Acc: 0.635597, Pre: 0.762066, Rec: 0.726708, F1: 0.740813\n",
            "Train F1: 0.843071\n",
            "epoch: 46, loss: 0.5245\n",
            "Test Loss: 0.897045, Acc: 0.633724, Pre: 0.748094, Rec: 0.743840, F1: 0.742936\n",
            "Train F1: 0.849929\n",
            "epoch: 47, loss: 0.4422\n",
            "Test Loss: 0.858715, Acc: 0.651522, Pre: 0.777835, Rec: 0.717706, F1: 0.741605\n",
            "Train F1: 0.852401\n",
            "epoch: 48, loss: 0.5713\n",
            "Test Loss: 0.893977, Acc: 0.632787, Pre: 0.756745, Rec: 0.723121, F1: 0.733457\n",
            "Train F1: 0.854706\n",
            "epoch: 49, loss: 0.5904\n",
            "Test Loss: 0.850188, Acc: 0.648244, Pre: 0.763118, Rec: 0.749453, F1: 0.755312\n",
            "Train F1: 0.856887\n",
            "epoch: 50, loss: 0.4342\n",
            "Test Loss: 0.866442, Acc: 0.644496, Pre: 0.764827, Rec: 0.735498, F1: 0.749195\n",
            "Train F1: 0.861620\n",
            "epoch: 51, loss: 0.5822\n",
            "Test Loss: 0.941804, Acc: 0.645433, Pre: 0.762734, Rec: 0.748274, F1: 0.738878\n",
            "Train F1: 0.858744\n",
            "epoch: 52, loss: 0.5044\n",
            "Test Loss: 0.888295, Acc: 0.645433, Pre: 0.752963, Rec: 0.767160, F1: 0.748927\n",
            "Train F1: 0.866125\n",
            "epoch: 53, loss: 0.5078\n",
            "Test Loss: 0.864518, Acc: 0.644965, Pre: 0.757595, Rec: 0.742720, F1: 0.749560\n",
            "Train F1: 0.867888\n",
            "epoch: 54, loss: 0.5994\n",
            "Test Loss: 0.872027, Acc: 0.666511, Pre: 0.786382, Rec: 0.743545, F1: 0.759639\n",
            "Train F1: 0.867394\n",
            "epoch: 55, loss: 0.451\n",
            "Test Loss: 0.869126, Acc: 0.665574, Pre: 0.760323, Rec: 0.791989, F1: 0.772279\n",
            "Train F1: 0.871649\n",
            "epoch: 56, loss: 0.5085\n",
            "Test Loss: 0.888899, Acc: 0.674941, Pre: 0.784391, Rec: 0.759823, F1: 0.764609\n",
            "Train F1: 0.869050\n",
            "epoch: 57, loss: 0.5348\n",
            "Test Loss: 0.838556, Acc: 0.671663, Pre: 0.763531, Rec: 0.784837, F1: 0.771513\n",
            "Train F1: 0.878827\n",
            "epoch: 58, loss: 0.5895\n",
            "Test Loss: 0.881104, Acc: 0.666511, Pre: 0.791058, Rec: 0.736411, F1: 0.756389\n",
            "Train F1: 0.879071\n",
            "epoch: 59, loss: 0.5416\n",
            "Test Loss: 0.872905, Acc: 0.666511, Pre: 0.767699, Rec: 0.769243, F1: 0.763298\n",
            "Train F1: 0.885110\n",
            "epoch: 60, loss: 0.5424\n",
            "Test Loss: 0.886518, Acc: 0.677752, Pre: 0.785197, Rec: 0.762802, F1: 0.764138\n",
            "Train F1: 0.883376\n",
            "epoch: 61, loss: 0.5029\n",
            "Test Loss: 0.859940, Acc: 0.674941, Pre: 0.773701, Rec: 0.777126, F1: 0.774301\n",
            "Train F1: 0.890416\n",
            "epoch: 62, loss: 0.4432\n",
            "Test Loss: 0.885635, Acc: 0.689461, Pre: 0.793504, Rec: 0.784354, F1: 0.774929\n",
            "Train F1: 0.891206\n",
            "epoch: 63, loss: 0.473\n",
            "Test Loss: 0.852713, Acc: 0.676347, Pre: 0.772226, Rec: 0.769279, F1: 0.766408\n",
            "Train F1: 0.888567\n",
            "epoch: 64, loss: 0.4135\n",
            "Test Loss: 0.917310, Acc: 0.678220, Pre: 0.800841, Rec: 0.742139, F1: 0.764805\n",
            "Train F1: 0.894127\n",
            "epoch: 65, loss: 0.52\n",
            "Test Loss: 0.898375, Acc: 0.675410, Pre: 0.810866, Rec: 0.727418, F1: 0.758814\n",
            "Train F1: 0.895795\n",
            "epoch: 66, loss: 0.4311\n",
            "Test Loss: 0.865380, Acc: 0.688993, Pre: 0.797399, Rec: 0.759699, F1: 0.774006\n",
            "Train F1: 0.893844\n",
            "epoch: 67, loss: 0.4529\n",
            "Test Loss: 0.884003, Acc: 0.689930, Pre: 0.784727, Rec: 0.789941, F1: 0.779564\n",
            "Train F1: 0.897498\n",
            "epoch: 68, loss: 0.4216\n",
            "Test Loss: 0.838066, Acc: 0.704918, Pre: 0.803311, Rec: 0.786481, F1: 0.788696\n",
            "Train F1: 0.901626\n",
            "epoch: 69, loss: 0.4591\n",
            "Test Loss: 0.868309, Acc: 0.707260, Pre: 0.818911, Rec: 0.760223, F1: 0.781680\n",
            "Train F1: 0.901403\n",
            "epoch: 70, loss: 0.3381\n",
            "Test Loss: 0.905757, Acc: 0.700234, Pre: 0.806485, Rec: 0.756443, F1: 0.773195\n",
            "Train F1: 0.904808\n",
            "epoch: 71, loss: 0.4874\n",
            "Test Loss: 0.897452, Acc: 0.702108, Pre: 0.803744, Rec: 0.775681, F1: 0.779067\n",
            "Train F1: 0.906629\n",
            "epoch: 72, loss: 0.3365\n",
            "Test Loss: 0.842767, Acc: 0.697424, Pre: 0.786257, Rec: 0.790041, F1: 0.786301\n",
            "Train F1: 0.911014\n",
            "epoch: 73, loss: 0.4379\n",
            "Test Loss: 0.867890, Acc: 0.692740, Pre: 0.781487, Rec: 0.790147, F1: 0.782904\n",
            "Train F1: 0.908011\n",
            "epoch: 74, loss: 0.414\n",
            "Test Loss: 0.881658, Acc: 0.697424, Pre: 0.805133, Rec: 0.768177, F1: 0.782940\n",
            "Train F1: 0.909286\n",
            "epoch: 75, loss: 0.2917\n",
            "Test Loss: 0.842654, Acc: 0.708665, Pre: 0.805882, Rec: 0.788671, F1: 0.793697\n",
            "Train F1: 0.915351\n",
            "epoch: 76, loss: 0.3813\n",
            "Test Loss: 0.860294, Acc: 0.715691, Pre: 0.807102, Rec: 0.794647, F1: 0.792829\n",
            "Train F1: 0.919465\n",
            "epoch: 77, loss: 0.3664\n",
            "Test Loss: 0.864079, Acc: 0.712881, Pre: 0.804965, Rec: 0.792245, F1: 0.791245\n",
            "Train F1: 0.916800\n",
            "epoch: 78, loss: 0.2958\n",
            "Test Loss: 0.949800, Acc: 0.705386, Pre: 0.821965, Rec: 0.759003, F1: 0.778230\n",
            "Train F1: 0.918427\n",
            "epoch: 79, loss: 0.2813\n",
            "Test Loss: 0.876077, Acc: 0.725059, Pre: 0.820648, Rec: 0.789319, F1: 0.797725\n",
            "Train F1: 0.917828\n",
            "epoch: 80, loss: 0.4318\n",
            "Test Loss: 0.921353, Acc: 0.693677, Pre: 0.782971, Rec: 0.802547, F1: 0.787197\n",
            "Train F1: 0.920995\n",
            "epoch: 81, loss: 0.3435\n",
            "Test Loss: 0.968524, Acc: 0.711007, Pre: 0.811699, Rec: 0.783366, F1: 0.785004\n",
            "Train F1: 0.923845\n",
            "epoch: 82, loss: 0.3402\n",
            "Test Loss: 0.877364, Acc: 0.719438, Pre: 0.823377, Rec: 0.780281, F1: 0.794858\n",
            "Train F1: 0.922326\n",
            "epoch: 83, loss: 0.3487\n",
            "Test Loss: 0.970677, Acc: 0.708197, Pre: 0.803594, Rec: 0.788667, F1: 0.781378\n",
            "Train F1: 0.924784\n",
            "epoch: 84, loss: 0.3017\n",
            "Test Loss: 0.905653, Acc: 0.714754, Pre: 0.801091, Rec: 0.804627, F1: 0.796027\n",
            "Train F1: 0.924708\n",
            "epoch: 85, loss: 0.3417\n",
            "Test Loss: 0.930891, Acc: 0.714286, Pre: 0.806802, Rec: 0.790804, F1: 0.792263\n",
            "Train F1: 0.930439\n",
            "epoch: 86, loss: 0.3801\n",
            "Test Loss: 0.933323, Acc: 0.719906, Pre: 0.810824, Rec: 0.791615, F1: 0.795715\n",
            "Train F1: 0.930897\n",
            "epoch: 87, loss: 0.2435\n",
            "Test Loss: 0.907159, Acc: 0.723653, Pre: 0.807984, Rec: 0.796559, F1: 0.793611\n",
            "Train F1: 0.934305\n",
            "epoch: 88, loss: 0.2261\n",
            "Test Loss: 0.895907, Acc: 0.731148, Pre: 0.818433, Rec: 0.797973, F1: 0.799509\n",
            "Train F1: 0.928016\n",
            "epoch: 89, loss: 0.3719\n",
            "Test Loss: 0.945351, Acc: 0.714754, Pre: 0.800519, Rec: 0.812766, F1: 0.798296\n",
            "Train F1: 0.932929\n",
            "epoch: 90, loss: 0.132\n",
            "Test Loss: 0.944626, Acc: 0.733489, Pre: 0.820230, Rec: 0.805284, F1: 0.799042\n",
            "Train F1: 0.931835\n",
            "epoch: 91, loss: 0.3211\n",
            "Test Loss: 0.928846, Acc: 0.724590, Pre: 0.820872, Rec: 0.794532, F1: 0.801323\n",
            "Train F1: 0.933765\n",
            "epoch: 92, loss: 0.2798\n",
            "Test Loss: 0.912587, Acc: 0.730211, Pre: 0.822415, Rec: 0.790566, F1: 0.802428\n",
            "Train F1: 0.938498\n",
            "epoch: 93, loss: 0.3476\n",
            "Test Loss: 1.051286, Acc: 0.722248, Pre: 0.826057, Rec: 0.787403, F1: 0.786872\n",
            "Train F1: 0.944126\n",
            "epoch: 94, loss: 0.324\n",
            "Test Loss: 0.938577, Acc: 0.735831, Pre: 0.828043, Rec: 0.800908, F1: 0.802049\n",
            "Train F1: 0.943479\n",
            "epoch: 95, loss: 0.284\n",
            "Test Loss: 1.024924, Acc: 0.725995, Pre: 0.818530, Rec: 0.791346, F1: 0.794688\n",
            "Train F1: 0.945648\n",
            "epoch: 96, loss: 0.1825\n",
            "Test Loss: 0.989177, Acc: 0.731148, Pre: 0.822227, Rec: 0.794620, F1: 0.799412\n",
            "Train F1: 0.944262\n",
            "epoch: 97, loss: 0.1949\n",
            "Test Loss: 0.966381, Acc: 0.719438, Pre: 0.798890, Rec: 0.811113, F1: 0.800182\n",
            "Train F1: 0.940587\n",
            "epoch: 98, loss: 0.2922\n",
            "Test Loss: 1.026730, Acc: 0.730679, Pre: 0.836048, Rec: 0.790536, F1: 0.795070\n",
            "Train F1: 0.943861\n",
            "epoch: 99, loss: 0.235\n",
            "Test Loss: 0.945929, Acc: 0.738642, Pre: 0.828379, Rec: 0.808914, F1: 0.810463\n",
            "The  0  th fold cross validation:\n",
            "Train F1: 0.047847\n",
            "epoch: 0, loss: 1.55\n",
            "Test Loss: 1.368754, Acc: 0.331148, Pre: 0.785347, Rec: 0.518152, F1: 0.477876\n",
            "Train F1: 0.489397\n",
            "epoch: 1, loss: 1.203\n",
            "Test Loss: 1.194004, Acc: 0.331616, Pre: 0.776243, Rec: 0.658652, F1: 0.497268\n",
            "Train F1: 0.493474\n",
            "epoch: 2, loss: 1.115\n",
            "Test Loss: 1.114938, Acc: 0.373302, Pre: 0.796501, Rec: 0.399811, F1: 0.421266\n",
            "Train F1: 0.464402\n",
            "epoch: 3, loss: 1.103\n",
            "Test Loss: 1.097348, Acc: 0.386885, Pre: 0.572012, Rec: 0.429158, F1: 0.448338\n",
            "Train F1: 0.515339\n",
            "epoch: 4, loss: 1.083\n",
            "Test Loss: 1.083972, Acc: 0.432787, Pre: 0.626906, Rec: 0.508429, F1: 0.541080\n",
            "Train F1: 0.541459\n",
            "epoch: 5, loss: 1.068\n",
            "Test Loss: 1.066483, Acc: 0.445433, Pre: 0.616612, Rec: 0.558812, F1: 0.575812\n",
            "Train F1: 0.585348\n",
            "epoch: 6, loss: 1.013\n",
            "Test Loss: 1.049405, Acc: 0.444028, Pre: 0.630916, Rec: 0.545809, F1: 0.565239\n",
            "Train F1: 0.595084\n",
            "epoch: 7, loss: 1.015\n",
            "Test Loss: 1.042669, Acc: 0.444496, Pre: 0.618731, Rec: 0.567973, F1: 0.584126\n",
            "Train F1: 0.615206\n",
            "epoch: 8, loss: 1.027\n",
            "Test Loss: 1.042354, Acc: 0.450585, Pre: 0.622835, Rec: 0.583635, F1: 0.600259\n",
            "Train F1: 0.624551\n",
            "epoch: 9, loss: 1.035\n",
            "Test Loss: 1.024766, Acc: 0.467916, Pre: 0.639761, Rec: 0.577924, F1: 0.603956\n",
            "Train F1: 0.633233\n",
            "epoch: 10, loss: 1.005\n",
            "Test Loss: 1.031275, Acc: 0.467447, Pre: 0.627281, Rec: 0.601261, F1: 0.612662\n",
            "Train F1: 0.649653\n",
            "epoch: 11, loss: 1.019\n",
            "Test Loss: 1.019787, Acc: 0.468384, Pre: 0.624978, Rec: 0.610629, F1: 0.617541\n",
            "Train F1: 0.659677\n",
            "epoch: 12, loss: 0.9965\n",
            "Test Loss: 1.022183, Acc: 0.484309, Pre: 0.639316, Rec: 0.648501, F1: 0.641088\n",
            "Train F1: 0.673455\n",
            "epoch: 13, loss: 0.9601\n",
            "Test Loss: 1.008187, Acc: 0.489930, Pre: 0.647910, Rec: 0.603315, F1: 0.621298\n",
            "Train F1: 0.683033\n",
            "epoch: 14, loss: 0.9289\n",
            "Test Loss: 0.974709, Acc: 0.530679, Pre: 0.681469, Rec: 0.626968, F1: 0.650658\n",
            "Train F1: 0.696025\n",
            "epoch: 15, loss: 1.065\n",
            "Test Loss: 0.961089, Acc: 0.539110, Pre: 0.691574, Rec: 0.632511, F1: 0.659291\n",
            "Train F1: 0.702885\n",
            "epoch: 16, loss: 0.8545\n",
            "Test Loss: 0.959588, Acc: 0.544731, Pre: 0.701772, Rec: 0.635944, F1: 0.664229\n",
            "Train F1: 0.712483\n",
            "epoch: 17, loss: 0.9075\n",
            "Test Loss: 0.940787, Acc: 0.548009, Pre: 0.684611, Rec: 0.683828, F1: 0.684088\n",
            "Train F1: 0.717113\n",
            "epoch: 18, loss: 0.8907\n",
            "Test Loss: 0.914664, Acc: 0.564871, Pre: 0.696048, Rec: 0.690411, F1: 0.693162\n",
            "Train F1: 0.727630\n",
            "epoch: 19, loss: 0.8938\n",
            "Test Loss: 0.928183, Acc: 0.558314, Pre: 0.702306, Rec: 0.661677, F1: 0.680063\n",
            "Train F1: 0.737484\n",
            "epoch: 20, loss: 0.9896\n",
            "Test Loss: 0.920963, Acc: 0.575644, Pre: 0.704381, Rec: 0.708356, F1: 0.698060\n",
            "Train F1: 0.747178\n",
            "epoch: 21, loss: 0.8689\n",
            "Test Loss: 0.888453, Acc: 0.595785, Pre: 0.732794, Rec: 0.694206, F1: 0.709617\n",
            "Train F1: 0.744725\n",
            "epoch: 22, loss: 0.9322\n",
            "Test Loss: 0.888966, Acc: 0.591101, Pre: 0.734705, Rec: 0.676196, F1: 0.703098\n",
            "Train F1: 0.751143\n",
            "epoch: 23, loss: 0.8424\n",
            "Test Loss: 0.890343, Acc: 0.592974, Pre: 0.738368, Rec: 0.682248, F1: 0.698683\n",
            "Train F1: 0.760900\n",
            "epoch: 24, loss: 0.7028\n",
            "Test Loss: 0.867171, Acc: 0.599063, Pre: 0.742380, Rec: 0.698682, F1: 0.719235\n",
            "Train F1: 0.759904\n",
            "epoch: 25, loss: 0.7087\n",
            "Test Loss: 0.875933, Acc: 0.614520, Pre: 0.751160, Rec: 0.692449, F1: 0.702548\n",
            "Train F1: 0.771437\n",
            "epoch: 26, loss: 0.7697\n",
            "Test Loss: 0.849976, Acc: 0.621546, Pre: 0.759276, Rec: 0.701230, F1: 0.721579\n",
            "Train F1: 0.774793\n",
            "epoch: 27, loss: 0.9683\n",
            "Test Loss: 0.847220, Acc: 0.636066, Pre: 0.754956, Rec: 0.735120, F1: 0.736868\n",
            "Train F1: 0.781000\n",
            "epoch: 28, loss: 0.6679\n",
            "Test Loss: 0.852500, Acc: 0.625761, Pre: 0.754691, Rec: 0.699834, F1: 0.716778\n",
            "Train F1: 0.783753\n",
            "epoch: 29, loss: 0.6363\n",
            "Test Loss: 0.867822, Acc: 0.622014, Pre: 0.744043, Rec: 0.708836, F1: 0.718527\n",
            "Train F1: 0.793100\n",
            "epoch: 30, loss: 0.8009\n",
            "Test Loss: 0.824633, Acc: 0.622482, Pre: 0.737046, Rec: 0.721799, F1: 0.728458\n",
            "Train F1: 0.794082\n",
            "epoch: 31, loss: 0.6938\n",
            "Test Loss: 0.820848, Acc: 0.653396, Pre: 0.763166, Rec: 0.724928, F1: 0.737981\n",
            "Train F1: 0.798179\n",
            "epoch: 32, loss: 0.7435\n",
            "Test Loss: 0.839846, Acc: 0.619204, Pre: 0.742354, Rec: 0.716766, F1: 0.729146\n",
            "Train F1: 0.807244\n",
            "epoch: 33, loss: 0.8208\n",
            "Test Loss: 0.897219, Acc: 0.600468, Pre: 0.719852, Rec: 0.718684, F1: 0.714251\n",
            "Train F1: 0.807329\n",
            "epoch: 34, loss: 0.7517\n",
            "Test Loss: 0.829482, Acc: 0.645902, Pre: 0.768376, Rec: 0.710400, F1: 0.728586\n",
            "Train F1: 0.810380\n",
            "epoch: 35, loss: 0.7425\n",
            "Test Loss: 0.798180, Acc: 0.648712, Pre: 0.751430, Rec: 0.744597, F1: 0.747586\n",
            "Train F1: 0.814389\n",
            "epoch: 36, loss: 0.5881\n",
            "Test Loss: 0.820837, Acc: 0.641218, Pre: 0.751633, Rec: 0.721583, F1: 0.732775\n",
            "Train F1: 0.816759\n",
            "epoch: 37, loss: 0.723\n",
            "Test Loss: 0.793076, Acc: 0.660422, Pre: 0.761402, Rec: 0.739169, F1: 0.746239\n",
            "Train F1: 0.820023\n",
            "epoch: 38, loss: 0.5452\n",
            "Test Loss: 0.756467, Acc: 0.675410, Pre: 0.773813, Rec: 0.756328, F1: 0.760918\n",
            "Train F1: 0.825358\n",
            "epoch: 39, loss: 0.7626\n",
            "Test Loss: 0.825490, Acc: 0.649649, Pre: 0.741676, Rec: 0.777096, F1: 0.749717\n",
            "Train F1: 0.831520\n",
            "epoch: 40, loss: 0.6056\n",
            "Test Loss: 0.765974, Acc: 0.680562, Pre: 0.790647, Rec: 0.755178, F1: 0.763005\n",
            "Train F1: 0.831598\n",
            "epoch: 41, loss: 0.5988\n",
            "Test Loss: 0.757273, Acc: 0.679625, Pre: 0.780638, Rec: 0.743819, F1: 0.749693\n",
            "Train F1: 0.841824\n",
            "epoch: 42, loss: 0.4655\n",
            "Test Loss: 0.747677, Acc: 0.689930, Pre: 0.788971, Rec: 0.742782, F1: 0.763116\n",
            "Train F1: 0.844559\n",
            "epoch: 43, loss: 0.5719\n",
            "Test Loss: 0.754285, Acc: 0.672600, Pre: 0.774291, Rec: 0.736944, F1: 0.750398\n",
            "Train F1: 0.844808\n",
            "epoch: 44, loss: 0.4915\n",
            "Test Loss: 0.804749, Acc: 0.652927, Pre: 0.759318, Rec: 0.718295, F1: 0.737690\n",
            "Train F1: 0.850958\n",
            "epoch: 45, loss: 0.6022\n",
            "Test Loss: 0.735682, Acc: 0.688056, Pre: 0.784107, Rec: 0.749816, F1: 0.760442\n",
            "Train F1: 0.855584\n",
            "epoch: 46, loss: 0.4895\n",
            "Test Loss: 0.743982, Acc: 0.700703, Pre: 0.799162, Rec: 0.753693, F1: 0.767416\n",
            "Train F1: 0.861118\n",
            "epoch: 47, loss: 0.5433\n",
            "Test Loss: 0.736838, Acc: 0.698361, Pre: 0.789460, Rec: 0.755019, F1: 0.766835\n",
            "Train F1: 0.857460\n",
            "epoch: 48, loss: 0.4566\n",
            "Test Loss: 0.738784, Acc: 0.698829, Pre: 0.798086, Rec: 0.761219, F1: 0.772889\n",
            "Train F1: 0.856211\n",
            "epoch: 49, loss: 0.4706\n",
            "Test Loss: 0.742314, Acc: 0.696487, Pre: 0.796519, Rec: 0.760079, F1: 0.765651\n",
            "Train F1: 0.864725\n",
            "epoch: 50, loss: 0.552\n",
            "Test Loss: 0.753551, Acc: 0.706323, Pre: 0.802639, Rec: 0.769755, F1: 0.766262\n",
            "Train F1: 0.866009\n",
            "epoch: 51, loss: 0.5365\n",
            "Test Loss: 0.768063, Acc: 0.687588, Pre: 0.782713, Rec: 0.745814, F1: 0.761590\n",
            "Train F1: 0.872554\n",
            "epoch: 52, loss: 0.5715\n",
            "Test Loss: 0.710746, Acc: 0.718970, Pre: 0.812160, Rec: 0.766476, F1: 0.782747\n",
            "Train F1: 0.873561\n",
            "epoch: 53, loss: 0.554\n",
            "Test Loss: 0.736852, Acc: 0.698361, Pre: 0.778928, Rec: 0.774046, F1: 0.771473\n",
            "Train F1: 0.873444\n",
            "epoch: 54, loss: 0.5242\n",
            "Test Loss: 0.713873, Acc: 0.714754, Pre: 0.816126, Rec: 0.757492, F1: 0.777235\n",
            "Train F1: 0.877806\n",
            "epoch: 55, loss: 0.4225\n",
            "Test Loss: 0.737025, Acc: 0.720843, Pre: 0.831907, Rec: 0.749049, F1: 0.773627\n",
            "Train F1: 0.880108\n",
            "epoch: 56, loss: 0.5055\n",
            "Test Loss: 0.756723, Acc: 0.711007, Pre: 0.810432, Rec: 0.774008, F1: 0.772935\n",
            "Train F1: 0.886586\n",
            "epoch: 57, loss: 0.3333\n",
            "Test Loss: 0.723274, Acc: 0.725059, Pre: 0.819501, Rec: 0.775095, F1: 0.788529\n",
            "Train F1: 0.881207\n",
            "epoch: 58, loss: 0.4645\n",
            "Test Loss: 0.725357, Acc: 0.715222, Pre: 0.809118, Rec: 0.759254, F1: 0.778081\n",
            "Train F1: 0.882567\n",
            "epoch: 59, loss: 0.4455\n",
            "Test Loss: 0.762236, Acc: 0.714754, Pre: 0.809068, Rec: 0.756325, F1: 0.766733\n",
            "Train F1: 0.888900\n",
            "epoch: 60, loss: 0.4851\n",
            "Test Loss: 0.777795, Acc: 0.718033, Pre: 0.837263, Rec: 0.741061, F1: 0.768675\n",
            "Train F1: 0.891214\n",
            "epoch: 61, loss: 0.373\n",
            "Test Loss: 0.765956, Acc: 0.721780, Pre: 0.811391, Rec: 0.764502, F1: 0.781543\n",
            "Train F1: 0.888930\n",
            "epoch: 62, loss: 0.3625\n",
            "Test Loss: 0.732785, Acc: 0.726932, Pre: 0.805265, Rec: 0.793356, F1: 0.786093\n",
            "Train F1: 0.893916\n",
            "epoch: 63, loss: 0.469\n",
            "Test Loss: 0.735782, Acc: 0.740984, Pre: 0.842208, Rec: 0.783988, F1: 0.790000\n",
            "Train F1: 0.894692\n",
            "epoch: 64, loss: 0.4231\n",
            "Test Loss: 0.714494, Acc: 0.741920, Pre: 0.832255, Rec: 0.783626, F1: 0.799213\n",
            "Train F1: 0.892447\n",
            "epoch: 65, loss: 0.4916\n",
            "Test Loss: 0.791378, Acc: 0.718970, Pre: 0.793831, Rec: 0.791950, F1: 0.781191\n",
            "Train F1: 0.897003\n",
            "epoch: 66, loss: 0.4413\n",
            "Test Loss: 0.757717, Acc: 0.731616, Pre: 0.820972, Rec: 0.783446, F1: 0.786170\n",
            "Train F1: 0.905598\n",
            "epoch: 67, loss: 0.3758\n",
            "Test Loss: 0.718490, Acc: 0.747073, Pre: 0.844764, Rec: 0.778484, F1: 0.799099\n",
            "Train F1: 0.900490\n",
            "epoch: 68, loss: 0.4195\n",
            "Test Loss: 0.779171, Acc: 0.713349, Pre: 0.801666, Rec: 0.775920, F1: 0.771239\n",
            "Train F1: 0.906736\n",
            "epoch: 69, loss: 0.2423\n",
            "Test Loss: 0.712007, Acc: 0.749415, Pre: 0.843012, Rec: 0.777915, F1: 0.800299\n",
            "Train F1: 0.907475\n",
            "epoch: 70, loss: 0.3081\n",
            "Test Loss: 0.752462, Acc: 0.740047, Pre: 0.842241, Rec: 0.773893, F1: 0.790068\n",
            "Train F1: 0.909046\n",
            "epoch: 71, loss: 0.4931\n",
            "Test Loss: 0.756841, Acc: 0.725059, Pre: 0.807057, Rec: 0.792307, F1: 0.790556\n",
            "Train F1: 0.914619\n",
            "epoch: 72, loss: 0.382\n",
            "Test Loss: 0.698754, Acc: 0.754567, Pre: 0.839758, Rec: 0.792725, F1: 0.808285\n",
            "Train F1: 0.915536\n",
            "epoch: 73, loss: 0.4323\n",
            "Test Loss: 0.738017, Acc: 0.755504, Pre: 0.839714, Rec: 0.808245, F1: 0.805307\n",
            "Train F1: 0.913910\n",
            "epoch: 74, loss: 0.5195\n",
            "Test Loss: 0.695397, Acc: 0.741920, Pre: 0.823518, Rec: 0.801701, F1: 0.806983\n",
            "Train F1: 0.917428\n",
            "epoch: 75, loss: 0.3715\n",
            "Test Loss: 0.783503, Acc: 0.745667, Pre: 0.833508, Rec: 0.801032, F1: 0.803072\n",
            "Train F1: 0.917855\n",
            "epoch: 76, loss: 0.2898\n",
            "Test Loss: 0.726413, Acc: 0.746136, Pre: 0.832187, Rec: 0.797172, F1: 0.803421\n",
            "Train F1: 0.916948\n",
            "epoch: 77, loss: 0.4296\n",
            "Test Loss: 0.745145, Acc: 0.747541, Pre: 0.839646, Rec: 0.794157, F1: 0.807733\n",
            "Train F1: 0.919136\n",
            "epoch: 78, loss: 0.25\n",
            "Test Loss: 0.759864, Acc: 0.750820, Pre: 0.829966, Rec: 0.795766, F1: 0.798763\n",
            "Train F1: 0.927714\n",
            "epoch: 79, loss: 0.2723\n",
            "Test Loss: 0.738886, Acc: 0.751756, Pre: 0.842073, Rec: 0.785485, F1: 0.805919\n",
            "Train F1: 0.926210\n",
            "epoch: 80, loss: 0.3862\n",
            "Test Loss: 0.702235, Acc: 0.770023, Pre: 0.855919, Rec: 0.807020, F1: 0.821151\n",
            "Train F1: 0.923147\n",
            "epoch: 81, loss: 0.2498\n",
            "Test Loss: 0.728653, Acc: 0.761593, Pre: 0.836156, Rec: 0.809910, F1: 0.812652\n",
            "Train F1: 0.925270\n",
            "epoch: 82, loss: 0.3071\n",
            "Test Loss: 0.727288, Acc: 0.758314, Pre: 0.835223, Rec: 0.812215, F1: 0.815755\n",
            "Train F1: 0.925253\n",
            "epoch: 83, loss: 0.2595\n",
            "Test Loss: 0.741532, Acc: 0.765340, Pre: 0.842562, Rec: 0.807205, F1: 0.810540\n",
            "Train F1: 0.934208\n",
            "epoch: 84, loss: 0.3353\n",
            "Test Loss: 0.786384, Acc: 0.762998, Pre: 0.851979, Rec: 0.794858, F1: 0.805485\n",
            "Train F1: 0.926948\n",
            "epoch: 85, loss: 0.3094\n",
            "Test Loss: 0.768488, Acc: 0.768618, Pre: 0.859933, Rec: 0.802411, F1: 0.816673\n",
            "Train F1: 0.930818\n",
            "epoch: 86, loss: 0.2521\n",
            "Test Loss: 0.773760, Acc: 0.762998, Pre: 0.848528, Rec: 0.798710, F1: 0.805219\n",
            "Train F1: 0.926604\n",
            "epoch: 87, loss: 0.2763\n",
            "Test Loss: 0.791532, Acc: 0.753630, Pre: 0.844961, Rec: 0.789398, F1: 0.805697\n",
            "Train F1: 0.927713\n",
            "epoch: 88, loss: 0.3437\n",
            "Test Loss: 0.739035, Acc: 0.757845, Pre: 0.836255, Rec: 0.799132, F1: 0.813568\n",
            "Train F1: 0.934594\n",
            "epoch: 89, loss: 0.3152\n",
            "Test Loss: 0.768762, Acc: 0.749415, Pre: 0.836976, Rec: 0.786864, F1: 0.805994\n",
            "Train F1: 0.928617\n",
            "epoch: 90, loss: 0.242\n",
            "Test Loss: 0.806777, Acc: 0.754098, Pre: 0.854673, Rec: 0.778065, F1: 0.801134\n",
            "Train F1: 0.932963\n",
            "epoch: 91, loss: 0.2404\n",
            "Test Loss: 0.764394, Acc: 0.760656, Pre: 0.852904, Rec: 0.795224, F1: 0.810727\n",
            "Train F1: 0.941180\n",
            "epoch: 92, loss: 0.1997\n",
            "Test Loss: 0.817988, Acc: 0.744262, Pre: 0.817108, Rec: 0.806094, F1: 0.803985\n",
            "Train F1: 0.941816\n",
            "epoch: 93, loss: 0.1092\n",
            "Test Loss: 0.840958, Acc: 0.752693, Pre: 0.825907, Rec: 0.809866, F1: 0.809699\n",
            "Train F1: 0.938981\n",
            "epoch: 94, loss: 0.2577\n",
            "Test Loss: 0.787904, Acc: 0.764403, Pre: 0.832920, Rec: 0.820878, F1: 0.816489\n",
            "Train F1: 0.944181\n",
            "epoch: 95, loss: 0.2142\n",
            "Test Loss: 0.828805, Acc: 0.756909, Pre: 0.847386, Rec: 0.788482, F1: 0.806319\n",
            "Train F1: 0.937974\n",
            "epoch: 96, loss: 0.1855\n",
            "Test Loss: 0.814669, Acc: 0.759719, Pre: 0.855236, Rec: 0.788936, F1: 0.809864\n",
            "Train F1: 0.938114\n",
            "epoch: 97, loss: 0.1826\n",
            "Test Loss: 0.747920, Acc: 0.770960, Pre: 0.840428, Rec: 0.810258, F1: 0.817482\n",
            "Train F1: 0.940530\n",
            "epoch: 98, loss: 0.2604\n",
            "Test Loss: 0.845897, Acc: 0.757845, Pre: 0.841465, Rec: 0.806804, F1: 0.807629\n",
            "Train F1: 0.938086\n",
            "epoch: 99, loss: 0.2215\n",
            "Test Loss: 0.772100, Acc: 0.772365, Pre: 0.842100, Rec: 0.824650, F1: 0.824500\n",
            "The  0  th fold cross validation:\n",
            "Train F1: 0.045691\n",
            "epoch: 0, loss: 1.471\n",
            "Test Loss: 1.331079, Acc: 0.341920, Pre: 0.666667, Rec: 0.333333, F1: 0.333333\n",
            "Train F1: 0.447312\n",
            "epoch: 1, loss: 1.202\n",
            "Test Loss: 1.199594, Acc: 0.322717, Pre: 0.607723, Rec: 0.644534, F1: 0.496044\n",
            "Train F1: 0.535513\n",
            "epoch: 2, loss: 1.112\n",
            "Test Loss: 1.120199, Acc: 0.359251, Pre: 0.651116, Rec: 0.509264, F1: 0.477046\n",
            "Train F1: 0.513985\n",
            "epoch: 3, loss: 1.107\n",
            "Test Loss: 1.103401, Acc: 0.355035, Pre: 0.565520, Rec: 0.430749, F1: 0.453073\n",
            "Train F1: 0.574188\n",
            "epoch: 4, loss: 1.071\n",
            "Test Loss: 1.096890, Acc: 0.356440, Pre: 0.551012, Rec: 0.497741, F1: 0.519584\n",
            "Train F1: 0.604291\n",
            "epoch: 5, loss: 1.056\n",
            "Test Loss: 1.117726, Acc: 0.367681, Pre: 0.568584, Rec: 0.463171, F1: 0.489836\n",
            "Train F1: 0.618111\n",
            "epoch: 6, loss: 0.989\n",
            "Test Loss: 1.137734, Acc: 0.372834, Pre: 0.567632, Rec: 0.454328, F1: 0.488511\n",
            "Train F1: 0.623143\n",
            "epoch: 7, loss: 1.022\n",
            "Test Loss: 1.116094, Acc: 0.374707, Pre: 0.561738, Rec: 0.506349, F1: 0.522115\n",
            "Train F1: 0.633448\n",
            "epoch: 8, loss: 1.007\n",
            "Test Loss: 1.142901, Acc: 0.391101, Pre: 0.580850, Rec: 0.493740, F1: 0.527994\n",
            "Train F1: 0.640989\n",
            "epoch: 9, loss: 0.9837\n",
            "Test Loss: 1.164252, Acc: 0.391569, Pre: 0.580311, Rec: 0.489585, F1: 0.524431\n",
            "Train F1: 0.650355\n",
            "epoch: 10, loss: 0.951\n",
            "Test Loss: 1.178404, Acc: 0.391569, Pre: 0.591222, Rec: 0.478546, F1: 0.519136\n",
            "Train F1: 0.658200\n",
            "epoch: 11, loss: 0.9438\n",
            "Test Loss: 1.171635, Acc: 0.393443, Pre: 0.587530, Rec: 0.508821, F1: 0.532704\n",
            "Train F1: 0.669819\n",
            "epoch: 12, loss: 0.9615\n",
            "Test Loss: 1.176863, Acc: 0.419204, Pre: 0.613683, Rec: 0.514472, F1: 0.552032\n",
            "Train F1: 0.680916\n",
            "epoch: 13, loss: 1.021\n",
            "Test Loss: 1.169628, Acc: 0.435597, Pre: 0.607923, Rec: 0.604744, F1: 0.589918\n",
            "Train F1: 0.697725\n",
            "epoch: 14, loss: 0.9574\n",
            "Test Loss: 1.230563, Acc: 0.451991, Pre: 0.649462, Rec: 0.556987, F1: 0.578283\n",
            "Train F1: 0.704515\n",
            "epoch: 15, loss: 0.9644\n",
            "Test Loss: 1.172380, Acc: 0.448244, Pre: 0.636593, Rec: 0.559330, F1: 0.589864\n",
            "Train F1: 0.717039\n",
            "epoch: 16, loss: 0.8613\n",
            "Test Loss: 1.211683, Acc: 0.464169, Pre: 0.645300, Rec: 0.585346, F1: 0.603822\n",
            "Train F1: 0.729232\n",
            "epoch: 17, loss: 0.8325\n",
            "Test Loss: 1.195050, Acc: 0.462295, Pre: 0.657849, Rec: 0.563367, F1: 0.595185\n",
            "Train F1: 0.732084\n",
            "epoch: 18, loss: 0.8378\n",
            "Test Loss: 1.249103, Acc: 0.471663, Pre: 0.655745, Rec: 0.579049, F1: 0.607053\n",
            "Train F1: 0.742496\n",
            "epoch: 19, loss: 0.7581\n",
            "Test Loss: 1.162506, Acc: 0.471194, Pre: 0.653405, Rec: 0.594242, F1: 0.618339\n",
            "Train F1: 0.747918\n",
            "epoch: 20, loss: 0.8805\n",
            "Test Loss: 1.262969, Acc: 0.490398, Pre: 0.695633, Rec: 0.591979, F1: 0.609157\n",
            "Train F1: 0.754543\n",
            "epoch: 21, loss: 0.814\n",
            "Test Loss: 1.174993, Acc: 0.474005, Pre: 0.673063, Rec: 0.582895, F1: 0.621024\n",
            "Train F1: 0.761354\n",
            "epoch: 22, loss: 0.7543\n",
            "Test Loss: 1.210280, Acc: 0.503513, Pre: 0.687445, Rec: 0.605065, F1: 0.628874\n",
            "Train F1: 0.765955\n",
            "epoch: 23, loss: 0.7052\n",
            "Test Loss: 1.222253, Acc: 0.504450, Pre: 0.698329, Rec: 0.593859, F1: 0.632393\n",
            "Train F1: 0.771033\n",
            "epoch: 24, loss: 0.7345\n",
            "Test Loss: 1.245604, Acc: 0.496019, Pre: 0.704093, Rec: 0.584332, F1: 0.626487\n",
            "Train F1: 0.775588\n",
            "epoch: 25, loss: 0.7374\n",
            "Test Loss: 1.173664, Acc: 0.496487, Pre: 0.697231, Rec: 0.566481, F1: 0.616016\n",
            "Train F1: 0.780855\n",
            "epoch: 26, loss: 0.8683\n",
            "Test Loss: 1.194364, Acc: 0.499766, Pre: 0.674002, Rec: 0.632292, F1: 0.638672\n",
            "Train F1: 0.792697\n",
            "epoch: 27, loss: 0.7572\n",
            "Test Loss: 1.144377, Acc: 0.516628, Pre: 0.685898, Rec: 0.618143, F1: 0.644905\n",
            "Train F1: 0.791285\n",
            "epoch: 28, loss: 0.6969\n",
            "Test Loss: 1.188225, Acc: 0.521311, Pre: 0.693334, Rec: 0.605805, F1: 0.640009\n",
            "Train F1: 0.802143\n",
            "epoch: 29, loss: 0.7573\n",
            "Test Loss: 1.183204, Acc: 0.523653, Pre: 0.704555, Rec: 0.600200, F1: 0.637451\n",
            "Train F1: 0.806405\n",
            "epoch: 30, loss: 0.5773\n",
            "Test Loss: 1.235891, Acc: 0.533489, Pre: 0.720413, Rec: 0.612411, F1: 0.642708\n",
            "Train F1: 0.803851\n",
            "epoch: 31, loss: 0.6267\n",
            "Test Loss: 1.207420, Acc: 0.531616, Pre: 0.692471, Rec: 0.648045, F1: 0.658191\n",
            "Train F1: 0.816349\n",
            "epoch: 32, loss: 0.6857\n",
            "Test Loss: 1.258032, Acc: 0.530679, Pre: 0.723343, Rec: 0.601998, F1: 0.647133\n",
            "Train F1: 0.817942\n",
            "epoch: 33, loss: 0.6022\n",
            "Test Loss: 1.178415, Acc: 0.554098, Pre: 0.730559, Rec: 0.619020, F1: 0.652504\n",
            "Train F1: 0.821778\n",
            "epoch: 34, loss: 0.6028\n",
            "Test Loss: 1.190203, Acc: 0.570492, Pre: 0.719221, Rec: 0.661939, F1: 0.673454\n",
            "Train F1: 0.822261\n",
            "epoch: 35, loss: 0.6509\n",
            "Test Loss: 1.207594, Acc: 0.558782, Pre: 0.724363, Rec: 0.648967, F1: 0.653606\n",
            "Train F1: 0.826762\n",
            "epoch: 36, loss: 0.517\n",
            "Test Loss: 1.233465, Acc: 0.562529, Pre: 0.712865, Rec: 0.677671, F1: 0.660505\n",
            "Train F1: 0.833326\n",
            "epoch: 37, loss: 0.6438\n",
            "Test Loss: 1.157571, Acc: 0.555035, Pre: 0.703784, Rec: 0.657277, F1: 0.662916\n",
            "Train F1: 0.839095\n",
            "epoch: 38, loss: 0.5459\n",
            "Test Loss: 1.198425, Acc: 0.565340, Pre: 0.699776, Rec: 0.686408, F1: 0.675313\n",
            "Train F1: 0.843861\n",
            "epoch: 39, loss: 0.535\n",
            "Test Loss: 1.267016, Acc: 0.573770, Pre: 0.732015, Rec: 0.673811, F1: 0.666970\n",
            "Train F1: 0.842459\n",
            "epoch: 40, loss: 0.5787\n",
            "Test Loss: 1.136451, Acc: 0.582670, Pre: 0.731977, Rec: 0.661291, F1: 0.685880\n",
            "Train F1: 0.846230\n",
            "epoch: 41, loss: 0.6921\n",
            "Test Loss: 1.080192, Acc: 0.590164, Pre: 0.729735, Rec: 0.685725, F1: 0.694835\n",
            "Train F1: 0.846466\n",
            "epoch: 42, loss: 0.5136\n",
            "Test Loss: 1.197304, Acc: 0.568618, Pre: 0.728027, Rec: 0.632782, F1: 0.664723\n",
            "Train F1: 0.855357\n",
            "epoch: 43, loss: 0.4047\n",
            "Test Loss: 1.077984, Acc: 0.590632, Pre: 0.706919, Rec: 0.696631, F1: 0.693551\n",
            "Train F1: 0.856762\n",
            "epoch: 44, loss: 0.5422\n",
            "Test Loss: 1.134147, Acc: 0.598595, Pre: 0.734732, Rec: 0.682174, F1: 0.687908\n",
            "Train F1: 0.859508\n",
            "epoch: 45, loss: 0.6328\n",
            "Test Loss: 1.153255, Acc: 0.585948, Pre: 0.744192, Rec: 0.649878, F1: 0.682310\n",
            "Train F1: 0.862203\n",
            "epoch: 46, loss: 0.4217\n",
            "Test Loss: 1.220502, Acc: 0.586885, Pre: 0.785550, Rec: 0.630949, F1: 0.676229\n",
            "Train F1: 0.865450\n",
            "epoch: 47, loss: 0.3805\n",
            "Test Loss: 1.131135, Acc: 0.595785, Pre: 0.756979, Rec: 0.652928, F1: 0.686117\n",
            "Train F1: 0.874051\n",
            "epoch: 48, loss: 0.6095\n",
            "Test Loss: 1.113825, Acc: 0.601405, Pre: 0.718927, Rec: 0.702223, F1: 0.691962\n",
            "Train F1: 0.871220\n",
            "epoch: 49, loss: 0.437\n",
            "Test Loss: 1.153767, Acc: 0.613115, Pre: 0.767393, Rec: 0.675110, F1: 0.689078\n",
            "Train F1: 0.871769\n",
            "epoch: 50, loss: 0.4834\n",
            "Test Loss: 1.233825, Acc: 0.609368, Pre: 0.753137, Rec: 0.683818, F1: 0.684663\n",
            "Train F1: 0.874067\n",
            "epoch: 51, loss: 0.4529\n",
            "Test Loss: 1.157790, Acc: 0.610773, Pre: 0.744409, Rec: 0.695169, F1: 0.694616\n",
            "Train F1: 0.870868\n",
            "epoch: 52, loss: 0.3729\n",
            "Test Loss: 1.152652, Acc: 0.609836, Pre: 0.767933, Rec: 0.669126, F1: 0.698862\n",
            "Train F1: 0.882779\n",
            "epoch: 53, loss: 0.4346\n",
            "Test Loss: 1.206493, Acc: 0.606557, Pre: 0.766395, Rec: 0.657136, F1: 0.694253\n",
            "Train F1: 0.882731\n",
            "epoch: 54, loss: 0.3812\n",
            "Test Loss: 1.257320, Acc: 0.593443, Pre: 0.759916, Rec: 0.641960, F1: 0.679821\n",
            "Train F1: 0.887008\n",
            "epoch: 55, loss: 0.3655\n",
            "Test Loss: 1.153472, Acc: 0.614520, Pre: 0.752765, Rec: 0.675762, F1: 0.700162\n",
            "Train F1: 0.890220\n",
            "epoch: 56, loss: 0.4738\n",
            "Test Loss: 1.133965, Acc: 0.620141, Pre: 0.764956, Rec: 0.680644, F1: 0.699612\n",
            "Train F1: 0.891466\n",
            "epoch: 57, loss: 0.411\n",
            "Test Loss: 1.228746, Acc: 0.602342, Pre: 0.758973, Rec: 0.663415, F1: 0.692642\n",
            "Train F1: 0.893658\n",
            "epoch: 58, loss: 0.4509\n",
            "Test Loss: 1.171448, Acc: 0.623888, Pre: 0.748523, Rec: 0.706528, F1: 0.706184\n",
            "Train F1: 0.898254\n",
            "epoch: 59, loss: 0.3922\n",
            "Test Loss: 1.207393, Acc: 0.640281, Pre: 0.783776, Rec: 0.712468, F1: 0.711446\n",
            "Train F1: 0.894595\n",
            "epoch: 60, loss: 0.429\n",
            "Test Loss: 1.212736, Acc: 0.628571, Pre: 0.781704, Rec: 0.673617, F1: 0.697814\n",
            "Train F1: 0.901222\n",
            "epoch: 61, loss: 0.4881\n",
            "Test Loss: 1.164009, Acc: 0.620141, Pre: 0.749656, Rec: 0.692982, F1: 0.707774\n",
            "Train F1: 0.902882\n",
            "epoch: 62, loss: 0.3197\n",
            "Test Loss: 1.226431, Acc: 0.627635, Pre: 0.753152, Rec: 0.719986, F1: 0.709047\n",
            "Train F1: 0.899949\n",
            "epoch: 63, loss: 0.3355\n",
            "Test Loss: 1.401523, Acc: 0.599063, Pre: 0.779776, Rec: 0.652170, F1: 0.683390\n",
            "Train F1: 0.902553\n",
            "epoch: 64, loss: 0.4378\n",
            "Test Loss: 1.204849, Acc: 0.638876, Pre: 0.781862, Rec: 0.696539, F1: 0.711422\n",
            "Train F1: 0.906069\n",
            "epoch: 65, loss: 0.4221\n",
            "Test Loss: 1.245450, Acc: 0.631850, Pre: 0.755688, Rec: 0.712913, F1: 0.705752\n",
            "Train F1: 0.910691\n",
            "epoch: 66, loss: 0.409\n",
            "Test Loss: 1.244717, Acc: 0.633724, Pre: 0.771596, Rec: 0.689290, F1: 0.706629\n",
            "Train F1: 0.907778\n",
            "epoch: 67, loss: 0.3691\n",
            "Test Loss: 1.227796, Acc: 0.630445, Pre: 0.768189, Rec: 0.683170, F1: 0.700558\n",
            "Train F1: 0.913606\n",
            "epoch: 68, loss: 0.3209\n",
            "Test Loss: 1.251309, Acc: 0.632787, Pre: 0.786971, Rec: 0.679292, F1: 0.705112\n",
            "Train F1: 0.915445\n",
            "epoch: 69, loss: 0.3064\n",
            "Test Loss: 1.386958, Acc: 0.616862, Pre: 0.783041, Rec: 0.670585, F1: 0.704528\n",
            "Train F1: 0.909228\n",
            "epoch: 70, loss: 0.4323\n",
            "Test Loss: 1.242092, Acc: 0.626698, Pre: 0.757310, Rec: 0.677630, F1: 0.704169\n",
            "Train F1: 0.917294\n",
            "epoch: 71, loss: 0.4513\n",
            "Test Loss: 1.396380, Acc: 0.641218, Pre: 0.791393, Rec: 0.690978, F1: 0.701695\n",
            "Train F1: 0.913833\n",
            "epoch: 72, loss: 0.2417\n",
            "Test Loss: 1.330328, Acc: 0.630913, Pre: 0.796181, Rec: 0.675859, F1: 0.709474\n",
            "Train F1: 0.921882\n",
            "epoch: 73, loss: 0.4237\n",
            "Test Loss: 1.394073, Acc: 0.607494, Pre: 0.789319, Rec: 0.647895, F1: 0.700309\n",
            "Train F1: 0.926189\n",
            "epoch: 74, loss: 0.3532\n",
            "Test Loss: 1.166729, Acc: 0.631850, Pre: 0.752041, Rec: 0.710559, F1: 0.717672\n",
            "Train F1: 0.922773\n",
            "epoch: 75, loss: 0.2559\n",
            "Test Loss: 1.354727, Acc: 0.654801, Pre: 0.784428, Rec: 0.701646, F1: 0.719462\n",
            "Train F1: 0.927985\n",
            "epoch: 76, loss: 0.308\n",
            "Test Loss: 1.303424, Acc: 0.643091, Pre: 0.777203, Rec: 0.697609, F1: 0.724099\n",
            "Train F1: 0.922145\n",
            "epoch: 77, loss: 0.3696\n",
            "Test Loss: 1.531782, Acc: 0.623419, Pre: 0.838992, Rec: 0.648649, F1: 0.703177\n",
            "Train F1: 0.924068\n",
            "epoch: 78, loss: 0.3433\n",
            "Test Loss: 1.429251, Acc: 0.646370, Pre: 0.809886, Rec: 0.693230, F1: 0.718131\n",
            "Train F1: 0.933944\n",
            "epoch: 79, loss: 0.294\n",
            "Test Loss: 1.489102, Acc: 0.617799, Pre: 0.804956, Rec: 0.645075, F1: 0.701018\n",
            "Train F1: 0.928166\n",
            "epoch: 80, loss: 0.2208\n",
            "Test Loss: 1.415780, Acc: 0.645902, Pre: 0.792081, Rec: 0.679719, F1: 0.709122\n",
            "Train F1: 0.930856\n",
            "epoch: 81, loss: 0.3916\n",
            "Test Loss: 1.478802, Acc: 0.633255, Pre: 0.790297, Rec: 0.683641, F1: 0.708838\n",
            "Train F1: 0.933807\n",
            "epoch: 82, loss: 0.2719\n",
            "Test Loss: 1.266498, Acc: 0.632319, Pre: 0.745116, Rec: 0.722131, F1: 0.718610\n",
            "Train F1: 0.933301\n",
            "epoch: 83, loss: 0.2075\n",
            "Test Loss: 1.440601, Acc: 0.646838, Pre: 0.768531, Rec: 0.713226, F1: 0.716787\n",
            "Train F1: 0.932123\n",
            "epoch: 84, loss: 0.3477\n",
            "Test Loss: 1.435797, Acc: 0.651522, Pre: 0.796209, Rec: 0.725499, F1: 0.712910\n",
            "Train F1: 0.929796\n",
            "epoch: 85, loss: 0.3407\n",
            "Test Loss: 1.353430, Acc: 0.648712, Pre: 0.774427, Rec: 0.712115, F1: 0.726067\n",
            "Train F1: 0.927013\n",
            "epoch: 86, loss: 0.2678\n",
            "Test Loss: 1.596456, Acc: 0.618735, Pre: 0.818162, Rec: 0.634738, F1: 0.691831\n",
            "Train F1: 0.932212\n",
            "epoch: 87, loss: 0.2227\n",
            "Test Loss: 1.505037, Acc: 0.651991, Pre: 0.801757, Rec: 0.701474, F1: 0.713894\n",
            "Train F1: 0.940229\n",
            "epoch: 88, loss: 0.2087\n",
            "Test Loss: 1.343889, Acc: 0.666511, Pre: 0.795786, Rec: 0.711758, F1: 0.727514\n",
            "Train F1: 0.940883\n",
            "epoch: 89, loss: 0.3325\n",
            "Test Loss: 1.391039, Acc: 0.651054, Pre: 0.787232, Rec: 0.734536, F1: 0.715636\n",
            "Train F1: 0.938626\n",
            "epoch: 90, loss: 0.3127\n",
            "Test Loss: 1.506436, Acc: 0.656674, Pre: 0.806860, Rec: 0.698887, F1: 0.722670\n",
            "Train F1: 0.940090\n",
            "epoch: 91, loss: 0.3333\n",
            "Test Loss: 1.451776, Acc: 0.653396, Pre: 0.800642, Rec: 0.692145, F1: 0.719214\n",
            "Train F1: 0.941559\n",
            "epoch: 92, loss: 0.2653\n",
            "Test Loss: 1.326853, Acc: 0.649180, Pre: 0.790674, Rec: 0.699200, F1: 0.725888\n",
            "Train F1: 0.942507\n",
            "epoch: 93, loss: 0.321\n",
            "Test Loss: 1.536109, Acc: 0.629977, Pre: 0.777565, Rec: 0.665195, F1: 0.704755\n",
            "Train F1: 0.942784\n",
            "epoch: 94, loss: 0.2795\n",
            "Test Loss: 1.538027, Acc: 0.642623, Pre: 0.785682, Rec: 0.682059, F1: 0.712453\n",
            "Train F1: 0.944327\n",
            "epoch: 95, loss: 0.2851\n",
            "Test Loss: 1.710596, Acc: 0.659953, Pre: 0.808922, Rec: 0.701439, F1: 0.715820\n",
            "Train F1: 0.950844\n",
            "epoch: 96, loss: 0.2032\n",
            "Test Loss: 1.466806, Acc: 0.655738, Pre: 0.788914, Rec: 0.706947, F1: 0.725120\n",
            "Train F1: 0.948582\n",
            "epoch: 97, loss: 0.3347\n",
            "Test Loss: 1.667122, Acc: 0.649649, Pre: 0.806977, Rec: 0.696191, F1: 0.714440\n",
            "Train F1: 0.940276\n",
            "epoch: 98, loss: 0.2102\n",
            "Test Loss: 1.483579, Acc: 0.663232, Pre: 0.798822, Rec: 0.692110, F1: 0.718986\n",
            "Train F1: 0.945535\n",
            "epoch: 99, loss: 0.3115\n",
            "Test Loss: 1.796651, Acc: 0.644965, Pre: 0.818553, Rec: 0.679790, F1: 0.713789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NQA08EUIFZf7",
        "outputId": "89db85a0-c568-4522-f875-5d3725abc06e"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/accuracy_list.csv').values).float()\n",
        "precision_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/precision_list.csv').values).float()\n",
        "recall_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/recall_list.csv').values).float()\n",
        "F1_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_list.csv').values).float()\n",
        "F1_list_train = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_list_train.csv').values).float()\n",
        "loss_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/loss_list.csv').values).float()\n",
        "train_loss_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/train_loss_list.csv').values).float()\n",
        "train_var_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/train_var_list.csv').values).float()\n",
        "test_var_list = torch.from_numpy(pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/test_var_list.csv').values).float()\n",
        "\n",
        "totEpoch = 100\n",
        "x = range(0, totEpoch)\n",
        "plt.figure(figsize=(14,3))\n",
        "grid = plt.GridSpec(3, 2, wspace=0.5, hspace=0.5)\n",
        "plt.subplot(grid[:,0])\n",
        "plt.plot(x, F1_list_train, color=\"b\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train F1 score')\n",
        "plt.plot(x, F1_list, color=\"r\", marker='o',markersize='1.5',markeredgecolor='r',markeredgewidth = 1.5, label = 'Test F1 score')\n",
        "plt.plot(x, train_var_list, color=\"g\", marker='o',markersize='1.5',markeredgecolor='g',markeredgewidth = 1.5, label = 'Train variance')\n",
        "#plt.plot(x, test_var_list, color=\"c\", marker='o',markersize='1.5',markeredgecolor='c',markeredgewidth = 1.5, label = 'Test variance')\n",
        "plt.legend()\n",
        "plt.title('Test F1 score vs epoches')\n",
        "plt.xlabel('epoches')\n",
        "plt.ylabel('F1 score')\n",
        "plt.subplot(grid[:,1])\n",
        "plt.plot(x, train_loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Train Loss')\n",
        "plt.plot(x, loss_list, color=\"red\", marker='o',markersize='1.5',markeredgecolor='b',markeredgewidth = 1.5, label = 'Test Loss')\n",
        "plt.legend()\n",
        "plt.title('Test loss vs epoches')\n",
        "plt.xlabel('epoches')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0worwj_HYWbJ",
        "outputId": "5f2e3dd4-ecb1-4efa-d2b7-22a51d5c632d"
      },
      "source": [
        "import pandas as pd\n",
        "F1_list = pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/F1_list.csv').values\n",
        "F1_list = F1_list.reshape(len(F1_list)).tolist()\n",
        "test_var_list = pd.read_csv('./drive/My Drive/Colab Notebooks/Sentiment Classification/test_var_list.csv').values\n",
        "test_var_list = test_var_list.reshape(len(test_var_list)).tolist()\n",
        "print(max(F1_list))\n",
        "test_var_list[F1_list.index(max(F1_list))]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7731749602948487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0016499259296241472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}